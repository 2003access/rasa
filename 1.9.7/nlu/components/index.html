
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Components</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/banner.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="Entity Extraction" href="../entity-extraction/" />
    <link rel="prev" title="Choosing a Pipeline" href="../choosing-a-pipeline/" />

  <!-- Google Tag Manager -->
  <script type="opt-in" data-type="application/javascript" data-name="analytics">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MMHSZCS');</script>
  <!-- End Google Tag Manager -->
   
  
  <meta itemprop="image" content="https://rasa.com/assets/img/facebook-og.png">
  <meta property="og:title" content="Components" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="https://rasa.com/assets/img/facebook-og.png" />
  <meta property="og:url" content="https://rasa.com/docs/rasa/nlu/components" />
  
    <meta name="description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline" />
    <meta itemprop="description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline">
    <meta name="twitter:description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline" />
    <meta property="og:description" content="Customize the components and parameters of Rasa’s Machine Learning based
Natural Language Understanding pipeline" />
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@Rasa_HQ">
  <meta name="twitter:title" content="Components">
  <meta name="twitter:creator" content="@Rasa_HQ">
  <meta name="twitter:image" content="https://rasa.com/assets/img/facebook-og.png">

  <link rel="stylesheet" href="../../_static/xq-light.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/fontawesome/css/fontawesome-all.css" type="text/css" />
  <link rel="stylesheet" type="text/css" href="https://rasa.com/assets/css/klaro.css">
  <script defer type="text/javascript" src="https://rasa.com/assets/js/klaro_config.js"></script>
  <script defer type="text/javascript" src="https://rasa.com/assets/js/klaro.js"></script>
  <script defer type="text/javascript" src="../../_static/ace/src-min-noconflict/ace.js"></script>
  <script defer type="text/javascript" src="../../_static/chatblock/rasa-chatblock.min.js"></script>
  <script type="text/javascript" src="https://storage.googleapis.com/docs-theme/clipboard.min.js"></script>
  
    <link rel="icon" sizes="192x192" href="../../_static/icon-192x192.png">
    <link rel="apple-touch-icon" href="../../_static/icon-192x192.png" />
  
  
    
      <link rel="canonical" href="https://rasa.com/docs/rasa/nlu/components/"/>
    
  


  </head><body>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe data-name="analytics" data-src="https://www.googletagmanager.com/ns.html?id=GTM-MMHSZCS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<div class="announce-bar" role="banner">
  These docs are for version 1.x of Rasa Open Source.
  <a href="https://rasa.com/docs/rasa/">Docs for the new version 2.0 can be found here.</a>
</div>

<div class="nav-top">
  <div class="nav-container">
    <ul class="main-nav nav">
      <li>
      <a href="https://rasa.com/docs/" class="brand-link">
          <img src="../../_static/rasa_logo.svg" width="80px" height="40px" title="Rasa logo" alt="Rasa logo">
    	    <span class="logo extension">docs</span>
    	</a>
      </li>
    </ul>
    <ul class="secondary-nav nav">
      <li>
        <a href="https://github.com/rasaHQ/" target="_blank"><button class="button btn-ghost white"> <i class="fab fa-github"></i>GitHub</button></a>
      </li>
      <li>
        <a href="https://forum.rasa.com" target="_blank"><button class="button"><i class="fas fa-comments"></i> Ask the Community</button></a>
      </li>
    </ul>
  </div>
</div>

  
    
      <div class="sidebar-extended"></div>
    
  

  
    
      <div class="document">
    
  

    
      
        
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/installation/">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/rasa-tutorial/">Tutorial: Rasa Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/building-assistants/">Tutorial: Building Assistants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/command-line-interface/">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/architecture/">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/messaging-and-voice-channels/">Messaging and Voice Channels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/testing-your-assistant/">Testing Your Assistant</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/setting-up-ci-cd/">Setting up CI/CD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/validate-files/">Validate Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/configuring-http-api/">Configuring the HTTP API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/how-to-deploy/">Deploying Your Rasa Assistant</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user-guide/cloud-storage/">Cloud Storage</a></li>
</ul>
<p class="caption"><span class="caption-text">NLU</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../about/">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../using-nlu-only/">Using NLU Only</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training-data-format/">Training Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../language-support/">Language Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../choosing-a-pipeline/">Choosing a Pipeline</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../entity-extraction/">Entity Extraction</a></li>
</ul>
<p class="caption"><span class="caption-text">Core</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../core/about/">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/stories/">Stories</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/domains/">Domains</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/responses/">Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/actions/">Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/reminders-and-external-events/">Reminders and External Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/policies/">Policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/slots/">Slots</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/forms/">Forms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/retrieval-actions/">Retrieval Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/interactive-learning/">Interactive Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/fallback-actions/">Fallback Actions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../core/knowledge-bases/">Knowledge Base Actions</a></li>
</ul>
<p class="caption"><span class="caption-text">Conversation Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/dialogue-elements/">Dialogue Elements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/small-talk/">Small Talk</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/completing-tasks/">Completing Tasks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dialogue-elements/guiding-users/">Guiding Users</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/action-server/">Action Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/http-api/">HTTP API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/jupyter-notebooks/">Jupyter Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/agent/">Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/custom-nlu-components/">Custom NLU Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/rasa-sdk/">Rasa SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/events/">Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/tracker/">Tracker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/tracker-stores/">Tracker Stores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/event-brokers/">Event Brokers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/lock-stores/">Lock Stores</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/training-data-importers/">Training Data Importers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/core-featurization/">Featurization of Conversations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/tensorflow_usage/">TensorFlow Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration-guide/">Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog/">Rasa Open Source Change Log</a></li>
</ul>
<p class="caption"><span class="caption-text">Migrate from (beta)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/google-dialogflow-to-rasa/">Dialogflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/facebook-wit-ai-to-rasa/">Wit.ai</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/microsoft-luis-to-rasa/">LUIS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migrate-from/ibm-watson-to-rasa/">IBM Watson</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Glossary</a></li>
</ul>

<div class="versions">
    <p class="caption">Versions</p>
    <div class="versions-content">
      <div>
        <span class="current-version">
          viewing: 1.9.7
        </span>
      </div>
      <div class="other-versions">
          <p>tags</p>
          <div class="dropdown-content">
              <a href="../../../1.10.25/nlu/components/">1.10.25</a>
              <a href="../../../1.10.24/nlu/components/">1.10.24</a>
              <a href="../../../1.10.23/nlu/components/">1.10.23</a>
              <a href="../../../1.10.22/nlu/components/">1.10.22</a>
              <a href="../../../1.10.21/nlu/components/">1.10.21</a>
              <a href="../../../1.10.20/nlu/components/">1.10.20</a>
              <a href="../../../1.10.19/nlu/components/">1.10.19</a>
              <a href="../../../1.10.18/nlu/components/">1.10.18</a>
              <a href="../../../1.10.17/nlu/components/">1.10.17</a>
              <a href="../../../1.10.16/nlu/components/">1.10.16</a>
              <a href="../../../1.10.15/nlu/components/">1.10.15</a>
              <a href="../../../1.10.14/nlu/components/">1.10.14</a>
              <a href="../../../1.10.13/nlu/components/">1.10.13</a>
              <a href="../../../1.10.12/nlu/components/">1.10.12</a>
              <a href="../../../1.10.11/nlu/components/">1.10.11</a>
              <a href="../../../1.10.10/nlu/components/">1.10.10</a>
              <a href="../../../1.10.9/nlu/components/">1.10.9</a>
              <a href="../../../1.10.8/nlu/components/">1.10.8</a>
              <a href="../../../1.10.7/nlu/components/">1.10.7</a>
              <a href="../../../1.10.6/nlu/components/">1.10.6</a>
              <a href="../../../1.10.5/nlu/components/">1.10.5</a>
              <a href="../../../1.10.4/nlu/components/">1.10.4</a>
              <a href="../../../1.10.3/nlu/components/">1.10.3</a>
              <a href="../../../1.10.2/nlu/components/">1.10.2</a>
              <a href="../../../1.10.1/nlu/components/">1.10.1</a>
              <a href="../../../1.10.0/nlu/components/">1.10.0</a>
              <a href="../../../1.9.7/nlu/components/">1.9.7</a>
              <a href="../../../1.9.6/nlu/components/">1.9.6</a>
              <a href="../../../1.9.5/nlu/components/">1.9.5</a>
              <a href="../../../1.9.4/nlu/components/">1.9.4</a>
              <a href="../../../1.9.3/nlu/components/">1.9.3</a>
              <a href="../../../1.9.2/nlu/components/">1.9.2</a>
              <a href="../../../1.9.1/nlu/components/">1.9.1</a>
              <a href="../../../1.9.0/nlu/components/">1.9.0</a>
              <a href="../../../1.8.3/nlu/components/">1.8.3</a>
              <a href="../../../1.8.2/nlu/components/">1.8.2</a>
              <a href="../../../1.8.1/nlu/components/">1.8.1</a>
              <a href="../../../1.8.0/nlu/components/">1.8.0</a>
              <a href="../../../1.7.4/nlu/components/">1.7.4</a>
              <a href="../../../1.7.3/nlu/components/">1.7.3</a>
              <a href="../../../1.7.2/nlu/components/">1.7.2</a>
              <a href="../../../1.7.1/nlu/components/">1.7.1</a>
              <a href="../../../1.7.0/nlu/components/">1.7.0</a>
              <a href="../../../1.6.2/nlu/components/">1.6.2</a>
              <a href="../../../1.5.3/nlu/components/">1.5.3</a>
              <a href="../../../1.4.6/nlu/components/">1.4.6</a>
              <a href="../../../1.3.10/nlu/components/">1.3.10</a>
              <a href="../../../1.2.9/nlu/components/">1.2.9</a>
          </div>
      </div>
    </div>
</div>


        </div>
      </div>
      
    
      
        
      
        <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  


    
    



    <p class="scv-banner"><a href="../../../1.10.25/nlu/components/"><b>Warning:</b> This document is for an old version of Rasa. The latest version is 1.10.25.</a></p>
<div class="section" id="components">
<span id="id1"></span><h1>Components<a class="headerlink" href="#components" title="Permalink to this headline">¶</a></h1>

  <div class="edit-link">
    <a class="reference external" href="https://github.com/RasaHQ/rasa/edit/master/docs/nlu/components.rst" target="_blank"><i class="fab fa-github" style="font-size: 85%; padding-right: 4px;"></i>SUGGEST EDITS</a>
  </div><p>This is a reference of the configuration options for every built-in
component in Rasa Open Source. If you want to build a custom component, check
out <a class="reference internal" href="../../api/custom-nlu-components/#custom-nlu-components"><span class="std std-ref">Custom NLU Components</span></a>.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#word-vector-sources" id="id26">Word Vector Sources</a></p>
<ul>
<li><p><a class="reference internal" href="#mitienlp" id="id27">MitieNLP</a></p></li>
<li><p><a class="reference internal" href="#spacynlp" id="id28">SpacyNLP</a></p></li>
<li><p><a class="reference internal" href="#hftransformersnlp" id="id29">HFTransformersNLP</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#tokenizers" id="id30">Tokenizers</a></p>
<ul>
<li><p><a class="reference internal" href="#whitespacetokenizer" id="id31">WhitespaceTokenizer</a></p></li>
<li><p><a class="reference internal" href="#jiebatokenizer" id="id32">JiebaTokenizer</a></p></li>
<li><p><a class="reference internal" href="#mitietokenizer" id="id33">MitieTokenizer</a></p></li>
<li><p><a class="reference internal" href="#spacytokenizer" id="id34">SpacyTokenizer</a></p></li>
<li><p><a class="reference internal" href="#converttokenizer" id="id35">ConveRTTokenizer</a></p></li>
<li><p><a class="reference internal" href="#languagemodeltokenizer" id="id36">LanguageModelTokenizer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#text-featurizers" id="id37">Text Featurizers</a></p>
<ul>
<li><p><a class="reference internal" href="#mitiefeaturizer" id="id38">MitieFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#spacyfeaturizer" id="id39">SpacyFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#convertfeaturizer" id="id40">ConveRTFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#languagemodelfeaturizer" id="id41">LanguageModelFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#regexfeaturizer" id="id42">RegexFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#countvectorsfeaturizer" id="id43">CountVectorsFeaturizer</a></p></li>
<li><p><a class="reference internal" href="#lexicalsyntacticfeaturizer" id="id44">LexicalSyntacticFeaturizer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#intent-classifiers" id="id45">Intent Classifiers</a></p>
<ul>
<li><p><a class="reference internal" href="#mitieintentclassifier" id="id46">MitieIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#sklearnintentclassifier" id="id47">SklearnIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#embeddingintentclassifier" id="id48">EmbeddingIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#keywordintentclassifier" id="id49">KeywordIntentClassifier</a></p></li>
<li><p><a class="reference internal" href="#dietclassifier" id="id50">DIETClassifier</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#entity-extractors" id="id51">Entity Extractors</a></p>
<ul>
<li><p><a class="reference internal" href="#mitieentityextractor" id="id52">MitieEntityExtractor</a></p></li>
<li><p><a class="reference internal" href="#spacyentityextractor" id="id53">SpacyEntityExtractor</a></p></li>
<li><p><a class="reference internal" href="#entitysynonymmapper" id="id54">EntitySynonymMapper</a></p></li>
<li><p><a class="reference internal" href="#crfentityextractor" id="id55">CRFEntityExtractor</a></p></li>
<li><p><a class="reference internal" href="#ducklinghttpextractor" id="id56">DucklingHTTPExtractor</a></p></li>
<li><p><a class="reference internal" href="#id20" id="id57">DIETClassifier</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#selectors" id="id58">Selectors</a></p>
<ul>
<li><p><a class="reference internal" href="#responseselector" id="id59">ResponseSelector</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#combined-entity-extractors-and-intent-classifiers" id="id60">Combined Entity Extractors and Intent Classifiers</a></p>
<ul>
<li><p><a class="reference internal" href="#diet-classifier" id="id61">DIETClassifier</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="word-vector-sources">
<h2><a class="toc-backref" href="#id26">Word Vector Sources</a><a class="headerlink" href="#word-vector-sources" title="Permalink to this headline">¶</a></h2>
<p>The following components load pre-trained models that are needed if you want to use pre-trained
word vectors in your pipeline.</p>
<div class="section" id="mitienlp">
<span id="id2"></span><h3><a class="toc-backref" href="#id27">MitieNLP</a><a class="headerlink" href="#mitienlp" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE initializer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>Nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Initializes MITIE structures. Every MITIE component relies on this,
hence this should be put at the beginning
of every pipeline that uses any MITIE components.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>The MITIE library needs a language model file, that <strong>must</strong> be specified in
the configuration:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieNLP&quot;</span>
  <span class="c1"># language model to load</span>
  <span class="nt">model</span><span class="p">:</span> <span class="s">&quot;data/total_word_feature_extractor.dat&quot;</span>
</pre></div>
</div>
<p>For more information where to get that file from, head over to
<a class="reference internal" href="../../user-guide/installation/#install-mitie"><span class="std std-ref">installing MITIE</span></a>.</p>
</dd>
</dl>
</div>
<div class="section" id="spacynlp">
<span id="id3"></span><h3><a class="toc-backref" href="#id28">SpacyNLP</a><a class="headerlink" href="#spacynlp" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>spaCy language initializer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>Nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Initializes spaCy structures. Every spaCy component relies on this, hence this should be put at the beginning
of every pipeline that uses any spaCy components.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>You need to specify the language model to use.
By default the language configured in the pipeline will be used as the language model name.
If the spaCy model to be used has a name that is different from the language tag (<code class="docutils literal notranslate"><span class="pre">&quot;en&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;de&quot;</span></code>, etc.),
the model name can be specified using the configuration variable <code class="docutils literal notranslate"><span class="pre">model</span></code>.
The name will be passed to <code class="docutils literal notranslate"><span class="pre">spacy.load(name)</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyNLP&quot;</span>
  <span class="c1"># language model to load</span>
  <span class="nt">model</span><span class="p">:</span> <span class="s">&quot;en_core_web_md&quot;</span>

  <span class="c1"># when retrieving word vectors, this will decide if the casing</span>
  <span class="c1"># of the word is relevant. E.g. `hello` and `Hello` will</span>
  <span class="c1"># retrieve the same vector, if set to `False`. For some</span>
  <span class="c1"># applications and models it makes sense to differentiate</span>
  <span class="c1"># between these two words, therefore setting this to `True`.</span>
  <span class="nt">case_sensitive</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
</pre></div>
</div>
<p>For more information on how to download the spaCy models, head over to
<a class="reference internal" href="../../user-guide/installation/#install-spacy"><span class="std std-ref">installing SpaCy</span></a>.</p>
</dd>
</dl>
</div>
<div class="section" id="hftransformersnlp">
<span id="id4"></span><h3><a class="toc-backref" href="#id29">HFTransformersNLP</a><a class="headerlink" href="#hftransformersnlp" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>HuggingFace’s Transformers based pre-trained language model initializer</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>Nothing</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Initializes specified pre-trained language model from HuggingFace’s <a class="reference external" href="https://huggingface.co/transformers/">Transformers library</a>.  The component applies language model specific tokenization and
featurization to compute sequence and sentence level representations for each example in the training data.
Include <a class="reference internal" href="#languagemodeltokenizer"><span class="std std-ref">LanguageModelTokenizer</span></a> and <a class="reference internal" href="#languagemodelfeaturizer"><span class="std std-ref">LanguageModelFeaturizer</span></a> to utilize the output of this
component for downstream NLU models.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">HFTransformersNLP</span></code> component, install Rasa Open Source with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">rasa[transformers]</span></code>.</p>
</div>
</div></blockquote>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>You should specify what language model to load via the parameter <code class="docutils literal notranslate"><span class="pre">model_name</span></code>. See the below table for the
available language models.
Additionally, you can also specify the architecture variation of the chosen language model by specifying the
parameter <code class="docutils literal notranslate"><span class="pre">model_weights</span></code>.
The full list of supported architectures can be found
<a class="reference external" href="https://huggingface.co/transformers/pretrained_models.html">here</a>.
If left empty, it uses the default model architecture that original Transformers library loads (see table below).</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>+----------------+--------------+-------------------------+
| Language Model | Parameter    | Default value for       |
|                | &quot;model_name&quot; | &quot;model_weights&quot;         |
+----------------+--------------+-------------------------+
| BERT           | bert         | bert-base-uncased       |
+----------------+--------------+-------------------------+
| GPT            | gpt          | openai-gpt              |
+----------------+--------------+-------------------------+
| GPT-2          | gpt2         | gpt2                    |
+----------------+--------------+-------------------------+
| XLNet          | xlnet        | xlnet-base-cased        |
+----------------+--------------+-------------------------+
| DistilBERT     | distilbert   | distilbert-base-uncased |
+----------------+--------------+-------------------------+
| RoBERTa        | roberta      | roberta-base            |
+----------------+--------------+-------------------------+
</pre></div>
</div>
<p>The following configuration loads the language model BERT:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">HFTransformersNLP</span>
    <span class="c1"># Name of the language model to use</span>
    <span class="nt">model_name</span><span class="p">:</span> <span class="s">&quot;bert&quot;</span>
    <span class="c1"># Pre-Trained weights to be loaded</span>
    <span class="nt">model_weights</span><span class="p">:</span> <span class="s">&quot;bert-base-uncased&quot;</span>

    <span class="c1"># An optional path to a specific directory to download and cache the pre-trained model weights.</span>
    <span class="c1"># The `default` cache_dir is the same as https://huggingface.co/transformers/serialization.html#cache-directory .</span>
    <span class="nt">cache_dir</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="tokenizers">
<span id="id5"></span><h2><a class="toc-backref" href="#id30">Tokenizers</a><a class="headerlink" href="#tokenizers" title="Permalink to this headline">¶</a></h2>
<p>Tokenizers split text into tokens.
If you want to split intents into multiple labels, e.g. for predicting multiple intents or for
modeling hierarchical intent structure, use the following flags with any tokenizer:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">intent_tokenization_flag</span></code> indicates whether to tokenize intent labels or not. Set it to <code class="docutils literal notranslate"><span class="pre">True</span></code>, so that intent
labels are tokenized.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">intent_split_symbol</span></code> sets the delimiter string to split the intent labels, default is underscore
(<code class="docutils literal notranslate"><span class="pre">_</span></code>).</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>All tokenizers add an additional token <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> to the end of the list of tokens when tokenizing
text and responses.</p>
</div>
</div></blockquote>
</li>
</ul>
<div class="section" id="whitespacetokenizer">
<span id="id6"></span><h3><a class="toc-backref" href="#id31">WhitespaceTokenizer</a><a class="headerlink" href="#whitespacetokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using whitespaces as a separator</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user messages, responses (if present), and intents (if specified)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates a token for every whitespace separated character sequence.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>Make the tokenizer case insensitive by adding the <code class="docutils literal notranslate"><span class="pre">case_sensitive:</span> <span class="pre">False</span></code> option, the
default being <code class="docutils literal notranslate"><span class="pre">case_sensitive:</span> <span class="pre">True</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;WhitespaceTokenizer&quot;</span>
  <span class="c1"># Flag to check whether to split intents</span>
  <span class="s">&quot;intent_tokenization_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
  <span class="c1"># Symbol on which intent should be split</span>
  <span class="s">&quot;intent_split_symbol&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_&quot;</span>
  <span class="c1"># Text will be tokenized with case sensitive as default</span>
  <span class="s">&quot;case_sensitive&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="jiebatokenizer">
<h3><a class="toc-backref" href="#id32">JiebaTokenizer</a><a class="headerlink" href="#jiebatokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using Jieba for Chinese language</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user messages, responses (if present), and intents (if specified)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the Jieba tokenizer specifically for Chinese
language. It will only work for the Chinese language.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code> you need to install Jieba with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">jieba</span></code>.</p>
</div>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>User’s custom dictionary files can be auto loaded by specifying the files’ directory path via <code class="docutils literal notranslate"><span class="pre">dictionary_path</span></code>.
If the <code class="docutils literal notranslate"><span class="pre">dictionary_path</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (the default), then no custom dictionary will be used.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;JiebaTokenizer&quot;</span>
  <span class="nt">dictionary_path</span><span class="p">:</span> <span class="s">&quot;path/to/custom/dictionary/dir&quot;</span>
  <span class="c1"># Flag to check whether to split intents</span>
  <span class="s">&quot;intent_tokenization_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
  <span class="c1"># Symbol on which intent should be split</span>
  <span class="s">&quot;intent_split_symbol&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="mitietokenizer">
<h3><a class="toc-backref" href="#id33">MitieTokenizer</a><a class="headerlink" href="#mitietokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using MITIE</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user messages, responses (if present), and intents (if specified)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a></p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the MITIE tokenizer.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieTokenizer&quot;</span>
  <span class="c1"># Flag to check whether to split intents</span>
  <span class="s">&quot;intent_tokenization_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
  <span class="c1"># Symbol on which intent should be split</span>
  <span class="s">&quot;intent_split_symbol&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="spacytokenizer">
<h3><a class="toc-backref" href="#id34">SpacyTokenizer</a><a class="headerlink" href="#spacytokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using spaCy</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user messages, responses (if present), and intents (if specified)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#spacynlp"><span class="std std-ref">SpacyNLP</span></a></p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the spaCy tokenizer.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyTokenizer&quot;</span>
  <span class="c1"># Flag to check whether to split intents</span>
  <span class="s">&quot;intent_tokenization_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
  <span class="c1"># Symbol on which intent should be split</span>
  <span class="s">&quot;intent_split_symbol&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="converttokenizer">
<span id="id7"></span><h3><a class="toc-backref" href="#id35">ConveRTTokenizer</a><a class="headerlink" href="#converttokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer using <a class="reference external" href="https://github.com/PolyAI-LDN/polyai-models#convert">ConveRT</a> model.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user messages, responses (if present), and intents (if specified)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the ConveRT tokenizer. Must be used whenever the <a class="reference internal" href="#convertfeaturizer"><span class="std std-ref">ConveRTFeaturizer</span></a> is used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since <code class="docutils literal notranslate"><span class="pre">ConveRT</span></code> model is trained only on an English corpus of conversations, this tokenizer should only
be used if your training data is in English language.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">ConveRTTokenizer</span></code>, install Rasa Open Source with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">rasa[convert]</span></code>.</p>
</div>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><p>Make the tokenizer case insensitive by adding the <code class="docutils literal notranslate"><span class="pre">case_sensitive:</span> <span class="pre">False</span></code> option, the
default being <code class="docutils literal notranslate"><span class="pre">case_sensitive:</span> <span class="pre">True</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;ConveRTTokenizer&quot;</span>
  <span class="c1"># Flag to check whether to split intents</span>
  <span class="s">&quot;intent_tokenization_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
  <span class="c1"># Symbol on which intent should be split</span>
  <span class="s">&quot;intent_split_symbol&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_&quot;</span>
  <span class="c1"># Text will be tokenized with case sensitive as default</span>
  <span class="s">&quot;case_sensitive&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="languagemodeltokenizer">
<span id="id8"></span><h3><a class="toc-backref" href="#id36">LanguageModelTokenizer</a><a class="headerlink" href="#languagemodeltokenizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Tokenizer from pre-trained language models</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user messages, responses (if present), and intents (if specified)</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#hftransformersnlp"><span class="std std-ref">HFTransformersNLP</span></a></p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>Creates tokens using the pre-trained language model specified in upstream <a class="reference internal" href="#hftransformersnlp"><span class="std std-ref">HFTransformersNLP</span></a> component.
Must be used whenever the <a class="reference internal" href="#languagemodelfeaturizer"><span class="std std-ref">LanguageModelFeaturizer</span></a> is used.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;LanguageModelTokenizer&quot;</span>
  <span class="c1"># Flag to check whether to split intents</span>
  <span class="s">&quot;intent_tokenization_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
  <span class="c1"># Symbol on which intent should be split</span>
  <span class="s">&quot;intent_split_symbol&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="text-featurizers">
<span id="id9"></span><h2><a class="toc-backref" href="#id37">Text Featurizers</a><a class="headerlink" href="#text-featurizers" title="Permalink to this headline">¶</a></h2>
<p>Text featurizers are divided into two different categories: sparse featurizers and dense featurizers.
Sparse featurizers are featurizers that return feature vectors with a lot of missing values, e.g. zeros.
As those feature vectors would normally take up a lot of memory, we store them as sparse features.
Sparse features only store the values that are non zero and their positions in the vector.
Thus, we save a lot of memory and are able to train on larger datasets.</p>
<p>By default all featurizers will return a matrix of length <code class="docutils literal notranslate"><span class="pre">(number-of-tokens</span> <span class="pre">x</span> <span class="pre">feature-dimension)</span></code>.
So, the returned matrix will have a feature vector for every token.
This allows us to train sequence models.
However, the additional token at the end (e.g. <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code>) contains features for the complete utterance.
This feature vector can be used in any bag-of-words model.
The corresponding classifier can therefore decide what kind of features to use.</p>
<div class="section" id="mitiefeaturizer">
<span id="id10"></span><h3><a class="toc-backref" href="#id38">MitieFeaturizer</a><a class="headerlink" href="#mitiefeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates a vector representation of user message and response (if specified) using the MITIE featurizer.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> for user messages and responses</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction, intent classification, and response classification using the MITIE
featurizer.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>NOT used by the <code class="docutils literal notranslate"><span class="pre">MitieIntentClassifier</span></code> component. But can be used by any component later in the pipeline
that makes use of <code class="docutils literal notranslate"><span class="pre">dense_features</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>The sentence vector, i.e. the vector of the <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> token, can be calculated in two different ways, either via
mean or via max pooling. You can specify the pooling method in your configuration file with the option <code class="docutils literal notranslate"><span class="pre">pooling</span></code>.
The default pooling method is set to <code class="docutils literal notranslate"><span class="pre">mean</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieFeaturizer&quot;</span>
  <span class="c1"># Specify what pooling operation should be used to calculate the vector of</span>
  <span class="c1"># the __CLS__ token. Available options: &#39;mean&#39; and &#39;max&#39;.</span>
  <span class="s">&quot;pooling&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;mean&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="spacyfeaturizer">
<span id="id11"></span><h3><a class="toc-backref" href="#id39">SpacyFeaturizer</a><a class="headerlink" href="#spacyfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates a vector representation of user message and response (if specified) using the spaCy featurizer.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> for user messages and responses</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#spacynlp"><span class="std std-ref">SpacyNLP</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction, intent classification, and response classification using the spaCy
featurizer.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>The sentence vector, i.e. the vector of the <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> token, can be calculated in two different ways, either via
mean or via max pooling. You can specify the pooling method in your configuration file with the option <code class="docutils literal notranslate"><span class="pre">pooling</span></code>.
The default pooling method is set to <code class="docutils literal notranslate"><span class="pre">mean</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyFeaturizer&quot;</span>
  <span class="c1"># Specify what pooling operation should be used to calculate the vector of</span>
  <span class="c1"># the __CLS__ token. Available options: &#39;mean&#39; and &#39;max&#39;.</span>
  <span class="s">&quot;pooling&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;mean&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="convertfeaturizer">
<span id="id12"></span><h3><a class="toc-backref" href="#id40">ConveRTFeaturizer</a><a class="headerlink" href="#convertfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates a vector representation of user message and response (if specified) using
<a class="reference external" href="https://github.com/PolyAI-LDN/polyai-models">ConveRT</a> model.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> for user messages and responses</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#converttokenizer"><span class="std std-ref">ConveRTTokenizer</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction, intent classification, and response selection.
It uses the <a class="reference external" href="https://github.com/PolyAI-LDN/polyai-models#tfhub-signatures">default signature</a> to compute vector
representations of input text.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since <code class="docutils literal notranslate"><span class="pre">ConveRT</span></code> model is trained only on an English corpus of conversations, this featurizer should only
be used if your training data is in English language.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">ConveRTTokenizer</span></code>, install Rasa Open Source with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">rasa[convert]</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;ConveRTFeaturizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="languagemodelfeaturizer">
<span id="id13"></span><h3><a class="toc-backref" href="#id41">LanguageModelFeaturizer</a><a class="headerlink" href="#languagemodelfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates a vector representation of user message and response (if specified) using a pre-trained language model.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> for user messages and responses</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#hftransformersnlp"><span class="std std-ref">HFTransformersNLP</span></a> and <a class="reference internal" href="#languagemodeltokenizer"><span class="std std-ref">LanguageModelTokenizer</span></a></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Dense featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction, intent classification, and response selection.
Uses the pre-trained language model specified in upstream <a class="reference internal" href="#hftransformersnlp"><span class="std std-ref">HFTransformersNLP</span></a> component to compute vector
representations of input text.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure that you use a language model which is pre-trained on the same language corpus as that of your
training data.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>Include <a class="reference internal" href="#hftransformersnlp"><span class="std std-ref">HFTransformersNLP</span></a> and <a class="reference internal" href="#languagemodeltokenizer"><span class="std std-ref">LanguageModelTokenizer</span></a> components before this component. Use
<a class="reference internal" href="#languagemodeltokenizer"><span class="std std-ref">LanguageModelTokenizer</span></a> to ensure tokens are correctly set for all components throughout the pipeline.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;LanguageModelFeaturizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="regexfeaturizer">
<span id="id14"></span><h3><a class="toc-backref" href="#id42">RegexFeaturizer</a><a class="headerlink" href="#regexfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates a vector representation of user message using regular expressions.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">sparse_features</span></code> for user messages and <code class="docutils literal notranslate"><span class="pre">tokens.pattern</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Sparse featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction and intent classification.
During training the <code class="docutils literal notranslate"><span class="pre">RegexFeaturizer</span></code> creates a list of regular expressions defined in the training
data format.
For each regex, a feature will be set marking whether this expression was found in the user message or not.
All features will later be fed into an intent classifier / entity extractor to simplify classification (assuming
the classifier has learned during the training phase, that this set feature indicates a certain intent / entity).
Regex features for entity extraction are currently only supported by the <a class="reference internal" href="#crfentityextractor"><span class="std std-ref">CRFEntityExtractor</span></a> and the
<a class="reference internal" href="#diet-classifier"><span class="std std-ref">DIETClassifier</span></a> components!</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;RegexFeaturizer&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="countvectorsfeaturizer">
<span id="id15"></span><h3><a class="toc-backref" href="#id43">CountVectorsFeaturizer</a><a class="headerlink" href="#countvectorsfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates bag-of-words representation of user messages, intents, and responses.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">sparse_features</span></code> for user messages, intents, and responses</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Sparse featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for intent classification and response selection.
Creates bag-of-words representation of user message, intent, and response using
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">sklearn’s CountVectorizer</a>.
All tokens which consist only of digits (e.g. 123 and 99 but not a123d) will be assigned to the same feature.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">sklearn’s CountVectorizer docs</a>
for detailed description of the configuration parameters.</p>
<p>This featurizer can be configured to use word or character n-grams, using the <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> configuration parameter.
By default <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> is set to <code class="docutils literal notranslate"><span class="pre">word</span></code> so word token counts are used as features.
If you want to use character n-grams, set <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> to <code class="docutils literal notranslate"><span class="pre">char</span></code> or <code class="docutils literal notranslate"><span class="pre">char_wb</span></code>.
The lower and upper boundaries of the n-grams can be configured via the parameters <code class="docutils literal notranslate"><span class="pre">min_ngram</span></code> and <code class="docutils literal notranslate"><span class="pre">max_ngram</span></code>.
By default both of them are set to <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Option <code class="docutils literal notranslate"><span class="pre">char_wb</span></code> creates character n-grams only from text inside word boundaries;
n-grams at the edges of words are padded with space.
This option can be used to create <a class="reference external" href="https://arxiv.org/abs/1810.07150">Subword Semantic Hashing</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For character n-grams do not forget to increase <code class="docutils literal notranslate"><span class="pre">min_ngram</span></code> and <code class="docutils literal notranslate"><span class="pre">max_ngram</span></code> parameters.
Otherwise the vocabulary will contain only single letters.</p>
</div>
<p>Handling Out-Of-Vocabulary (OOV) words:</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Enabled only if <code class="docutils literal notranslate"><span class="pre">analyzer</span></code> is <code class="docutils literal notranslate"><span class="pre">word</span></code>.</p>
</div>
<p>Since the training is performed on limited vocabulary data, it cannot be guaranteed that during prediction
an algorithm will not encounter an unknown word (a word that were not seen during training).
In order to teach an algorithm how to treat unknown words, some words in training data can be substituted
by generic word <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code>.
In this case during prediction all unknown words will be treated as this generic word <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code>.</p>
<p>For example, one might create separate intent <code class="docutils literal notranslate"><span class="pre">outofscope</span></code> in the training data containing messages of
different number of <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> s and maybe some additional general words.
Then an algorithm will likely classify a message with unknown words as this intent <code class="docutils literal notranslate"><span class="pre">outofscope</span></code>.</p>
<p>You can either set the <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> or a list of words <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> set a keyword for unseen words; if training data contains <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> as words in some
messages, during prediction the words that were not seen during training will be substituted with
provided <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code>; if <code class="docutils literal notranslate"><span class="pre">OOV_token=None</span></code> (default behaviour) words that were not seen during
training will be ignored during prediction time;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> set a list of words to be treated as <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> during training; if a list of words
that should be treated as Out-Of-Vocabulary is known, it can be set to <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> instead of manually
changing it in training data or using custom preprocessor.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This featurizer creates a bag-of-words representation by <strong>counting</strong> words,
so the number of <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> in the sentence might be important.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Providing <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> is optional, training data can contain <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> input manually or by custom
additional preprocessor.
Unseen words will be substituted with <code class="docutils literal notranslate"><span class="pre">OOV_token</span></code> <strong>only</strong> if this token is present in the training
data or <code class="docutils literal notranslate"><span class="pre">OOV_words</span></code> list is provided.</p>
</div>
</div></blockquote>
<p>If you want to share the vocabulary between user messages and intents, you need to set the option
<code class="docutils literal notranslate"><span class="pre">use_shared_vocab</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code>. In that case a common vocabulary set between tokens in intents and user messages
is build.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;CountVectorsFeaturizer&quot;</span>
  <span class="c1"># Analyzer to use, either &#39;word&#39;, &#39;char&#39;, or &#39;char_wb&#39;</span>
  <span class="s">&quot;analyzer&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;word&quot;</span>
  <span class="c1"># Set the lower and upper boundaries for the n-grams</span>
  <span class="s">&quot;min_ngram&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
  <span class="s">&quot;max_ngram&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
  <span class="c1"># Set the out-of-vocabulary token</span>
  <span class="s">&quot;OOV_token&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;_oov_&quot;</span>
  <span class="c1"># Whether to use a shared vocab</span>
  <span class="s">&quot;use_shared_vocab&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>
</pre></div>
</div>
<div class="toggle docutils container">
<div class="header docutils container">
<p>The above configuration parameters are the ones you should configure to fit your model to your data.
However, additional parameters exist that can be adapted.</p>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>+-------------------+-------------------+--------------------------------------------------------------+
| Parameter         | Default Value     | Description                                                  |
+===================+===================+==============================================================+
| use_shared_vocab  | False             | If set to &#39;True&#39; a common vocabulary is used for labels      |
|                   |                   | and user message.                                            |
+-------------------+-------------------+--------------------------------------------------------------+
| analyzer          | word              | Whether the features should be made of word n-gram or        |
|                   |                   | character n-grams. Option ‘char_wb’ creates character        |
|                   |                   | n-grams only from text inside word boundaries;               |
|                   |                   | n-grams at the edges of words are padded with space.         |
|                   |                   | Valid values: &#39;word&#39;, &#39;char&#39;, &#39;char_wb&#39;.                     |
+-------------------+-------------------+--------------------------------------------------------------+
| token_pattern     | r&quot;(?u)\b\w\w+\b&quot;  | Regular expression used to detect tokens.                    |
|                   |                   | Only used if &#39;analyzer&#39; is set to &#39;word&#39;.                    |
+-------------------+-------------------+--------------------------------------------------------------+
| strip_accents     | None              | Remove accents during the pre-processing step.               |
|                   |                   | Valid values: &#39;ascii&#39;, &#39;unicode&#39;, &#39;None&#39;.                    |
+-------------------+-------------------+--------------------------------------------------------------+
| stop_words        | None              | A list of stop words to use.                                 |
|                   |                   | Valid values: &#39;english&#39; (uses an internal list of            |
|                   |                   | English stop words), a list of custom stop words, or         |
|                   |                   | &#39;None&#39;.                                                      |
+-------------------+-------------------+--------------------------------------------------------------+
| min_df            | 1                 | When building the vocabulary ignore terms that have a        |
|                   |                   | document frequency strictly lower than the given threshold.  |
+-------------------+-------------------+--------------------------------------------------------------+
| max_df            | 1                 | When building the vocabulary ignore terms that have a        |
|                   |                   | document frequency strictly higher than the given threshold  |
|                   |                   | (corpus-specific stop words).                                |
+-------------------+-------------------+--------------------------------------------------------------+
| min_ngram         | 1                 | The lower boundary of the range of n-values for different    |
|                   |                   | word n-grams or char n-grams to be extracted.                |
+-------------------+-------------------+--------------------------------------------------------------+
| max_ngram         | 1                 | The upper boundary of the range of n-values for different    |
|                   |                   | word n-grams or char n-grams to be extracted.                |
+-------------------+-------------------+--------------------------------------------------------------+
| max_features      | None              | If not &#39;None&#39;, build a vocabulary that only consider the top |
|                   |                   | max_features ordered by term frequency across the corpus.    |
+-------------------+-------------------+--------------------------------------------------------------+
| lowercase         | True              | Convert all characters to lowercase before tokenizing.       |
+-------------------+-------------------+--------------------------------------------------------------+
| OOV_token         | None              | Keyword for unseen words.                                    |
+-------------------+-------------------+--------------------------------------------------------------+
| OOV_words         | []                | List of words to be treated as &#39;OOV_token&#39; during training.  |
+-------------------+-------------------+--------------------------------------------------------------+
</pre></div>
</div>
</div>
</dd>
</dl>
</div>
<div class="section" id="lexicalsyntacticfeaturizer">
<span id="id16"></span><h3><a class="toc-backref" href="#id44">LexicalSyntacticFeaturizer</a><a class="headerlink" href="#lexicalsyntacticfeaturizer" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Creates lexical and syntactic features for a user message to support entity extraction.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">sparse_features</span></code> for user messages</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code></p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>Sparse featurizer</p>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Creates features for entity extraction.
Moves with a sliding window over every token in the user message and creates features according to the
configuration (see below). As a default configuration is present, you don’t need to specify a configuration.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>You can configure what kind of lexical and syntactic features the featurizer should extract.
The following features are available:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>==============  ==========================================================================================
Feature Name    Description
==============  ==========================================================================================
BOS             Checks if the token is at the beginning of the sentence.
EOS             Checks if the token is at the end of the sentence.
low             Checks if the token is lower case.
upper           Checks if the token is upper case.
title           Checks if the token starts with an uppercase character and all remaining characters are
                lowercased.
digit           Checks if the token contains just digits.
prefix5         Take the first five characters of the token.
prefix2         Take the first two characters of the token.
suffix5         Take the last five characters of the token.
suffix3         Take the last three characters of the token.
suffix2         Take the last two characters of the token.
suffix1         Take the last character of the token.
pos             Take the Part-of-Speech tag of the token (``SpacyTokenizer`` required).
pos2            Take the first two characters of the Part-of-Speech tag of the token
                (``SpacyTokenizer`` required).
==============  ==========================================================================================
</pre></div>
</div>
<p>As the featurizer is moving over the tokens in a user message with a sliding window, you can define features for
previous tokens, the current token, and the next tokens in the sliding window.
You define the features as a [before, token, after] array.
If you want to define features for the token before, the current token, and the token after,
your features configuration would look like this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">LexicalSyntacticFeaturizer</span>
  <span class="s">&quot;features&quot;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span>
    <span class="p p-Indicator">[</span><span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;title&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;upper&quot;</span><span class="p p-Indicator">],</span>
    <span class="p p-Indicator">[</span><span class="s">&quot;BOS&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;EOS&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;upper&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;title&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;digit&quot;</span><span class="p p-Indicator">],</span>
    <span class="p p-Indicator">[</span><span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;title&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;upper&quot;</span><span class="p p-Indicator">],</span>
  <span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>This configuration is also the default configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to make use of <code class="docutils literal notranslate"><span class="pre">pos</span></code> or <code class="docutils literal notranslate"><span class="pre">pos2</span></code> you need to add <code class="docutils literal notranslate"><span class="pre">SpacyTokenizer</span></code> to your pipeline.</p>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="intent-classifiers">
<h2><a class="toc-backref" href="#id45">Intent Classifiers</a><a class="headerlink" href="#intent-classifiers" title="Permalink to this headline">¶</a></h2>
<p>Intent classifiers assign one of the intents defined in the domain file to incoming user messages.</p>
<div class="section" id="mitieintentclassifier">
<h3><a class="toc-backref" href="#id46">MitieIntentClassifier</a><a class="headerlink" href="#mitieintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE intent classifier (using a
<a class="reference external" href="https://github.com/mit-nlp/MITIE/blob/master/examples/python/text_categorizer_pure_model.py">text categorizer</a>)</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> for user message and <a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a></p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.98343</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This classifier uses MITIE to perform intent classification. The underlying classifier
is using a multi-class linear SVM with a sparse linear kernel (see
<a class="reference external" href="https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/text_categorizer_trainer.cpp#L222">MITIE trainer code</a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This classifier does not rely on any featurizer as it extracts features on its own.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieIntentClassifier&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="sklearnintentclassifier">
<h3><a class="toc-backref" href="#id47">SklearnIntentClassifier</a><a class="headerlink" href="#sklearnintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Sklearn intent classifier</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code> and <code class="docutils literal notranslate"><span class="pre">intent_ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> for user messages</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.78343</span><span class="p">},</span>
    <span class="nt">&quot;intent_ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.1485910906220309</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;goodbye&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.08161531595656784</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;restaurant_search&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>The sklearn intent classifier trains a linear SVM which gets optimized using a grid search. It also provides
rankings of the labels that did not “win”. The <code class="docutils literal notranslate"><span class="pre">SklearnIntentClassifier</span></code> needs to be preceded by a dense
featurizer in the pipeline. This dense featurizer creates the features used for the classification.
For more information about the algorithm itself, take a look at the
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>
documentation.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>During the training of the SVM a hyperparameter search is run to find the best parameter set.
In the configuration you can specify the parameters that will get tried.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SklearnIntentClassifier&quot;</span>
  <span class="c1"># Specifies the list of regularization values to</span>
  <span class="c1"># cross-validate over for C-SVM.</span>
  <span class="c1"># This is used with the ``kernel`` hyperparameter in GridSearchCV.</span>
  <span class="nt">C</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">1</span><span class="p p-Indicator">,</span> <span class="nv">2</span><span class="p p-Indicator">,</span> <span class="nv">5</span><span class="p p-Indicator">,</span> <span class="nv">10</span><span class="p p-Indicator">,</span> <span class="nv">20</span><span class="p p-Indicator">,</span> <span class="nv">100</span><span class="p p-Indicator">]</span>
  <span class="c1"># Specifies the kernel to use with C-SVM.</span>
  <span class="c1"># This is used with the ``C`` hyperparameter in GridSearchCV.</span>
  <span class="nt">kernels</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&quot;linear&quot;</span><span class="p p-Indicator">]</span>
  <span class="c1"># Gamma parameter of the C-SVM.</span>
  <span class="s">&quot;gamma&quot;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="nv">0.1</span><span class="p p-Indicator">]</span>
  <span class="c1"># We try to find a good number of cross folds to use during</span>
  <span class="c1"># intent training, this specifies the max number of folds.</span>
  <span class="s">&quot;max_cross_validation_folds&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>
  <span class="c1"># Scoring function used for evaluating the hyper parameters.</span>
  <span class="c1"># This can be a name or a function.</span>
  <span class="s">&quot;scoring_function&quot;</span><span class="p p-Indicator">:</span> <span class="s">&quot;f1_weighted&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="embeddingintentclassifier">
<span id="embedding-intent-classifier"></span><h3><a class="toc-backref" href="#id48">EmbeddingIntentClassifier</a><a class="headerlink" href="#embeddingintentclassifier" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier</span></code> is deprecated and should be replaced by <code class="docutils literal notranslate"><span class="pre">DIETClassifier</span></code>. See
<a class="reference internal" href="../../migration-guide/#migration-to-rasa-1-8"><span class="std std-ref">migration guide</span></a> for more details.</p>
</div>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Embedding intent classifier for intent classification</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code> and <code class="docutils literal notranslate"><span class="pre">intent_ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> and/or <code class="docutils literal notranslate"><span class="pre">sparse_features</span></code> for user messages, and optionally the intent</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.78343</span><span class="p">},</span>
    <span class="nt">&quot;intent_ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.1485910906220309</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;goodbye&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.08161531595656784</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;restaurant_search&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>The <code class="docutils literal notranslate"><span class="pre">EmbeddingIntentClassifier</span></code> embeds user inputs and intent labels into the same space.
Supervised embeddings are trained by maximizing similarity between them.
This algorithm is based on <a class="reference external" href="https://arxiv.org/abs/1709.03856">StarSpace</a>.
However, in this implementation the loss function is slightly different and
additional hidden layers are added together with dropout.
This algorithm also provides similarity rankings of the labels that did not “win”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If during prediction time a message contains <strong>only</strong> words unseen during training
and no Out-Of-Vocabulary preprocessor was used, an empty intent <code class="docutils literal notranslate"><span class="pre">None</span></code> is predicted with confidence
<code class="docutils literal notranslate"><span class="pre">0.0</span></code>. This might happen if you only use the <a class="reference internal" href="#countvectorsfeaturizer"><span class="std std-ref">CountVectorsFeaturizer</span></a> with a <code class="docutils literal notranslate"><span class="pre">word</span></code> analyzer
as featurizer. If you use the <code class="docutils literal notranslate"><span class="pre">char_wb</span></code> analyzer, you should always get an intent with a confidence
value <code class="docutils literal notranslate"><span class="pre">&gt;</span> <span class="pre">0.0</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>You can define a number of hyperparameters to adapt the model.
If you want to adapt your model, start by modifying the following parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code>:
This parameter sets the number of times the algorithm will see the training data (default: <code class="docutils literal notranslate"><span class="pre">300</span></code>).
One <code class="docutils literal notranslate"><span class="pre">epoch</span></code> is equals to one forward pass and one backward pass of all the training examples.
Sometimes the model needs more epochs to properly learn.
Sometimes more epochs don’t influence the performance.
The lower the number of epochs the faster the model is trained.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes</span></code>:
This parameter allows you to define the number of feed forward layers and their output
dimensions for user messages and intents (default: <code class="docutils literal notranslate"><span class="pre">text:</span> <span class="pre">[256,</span> <span class="pre">128],</span> <span class="pre">label:</span> <span class="pre">[]</span></code>).
Every entry in the list corresponds to a feed forward layer.
For example, if you set <code class="docutils literal notranslate"><span class="pre">text:</span> <span class="pre">[256,</span> <span class="pre">128]</span></code>, we will add two feed forward layers in front of
the transformer. The vectors of the input tokens (coming from the user message) will be passed on to those
layers. The first layer will have an output dimension of 256 and the second layer will have an output
dimension of 128. If an empty list is used (default behaviour), no feed forward layer will be
added.
Make sure to use only positive integer values. Usually, numbers of power of two are used.
Also, it is usual practice to have decreasing values in the list: next value is smaller or equal to the
value before.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_dimension</span></code>:
This parameter defines the output dimension of the embedding layers used inside the model (default: <code class="docutils literal notranslate"><span class="pre">20</span></code>).
We are using multiple embeddings layers inside the model architecture.
For example, the vector of the <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> token and the intent is passed on to an embedding layer before
they are compared and the loss is calculated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code>:
This parameter defines the fraction of kernel weights that are set to 0 for all feed forward layers
in the model (default: <code class="docutils literal notranslate"><span class="pre">0.0</span></code>). The value should be between 0 and 1. If you set <code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code>
to 0, no kernel weights will be set to 0, the layer acts as a standard feed forward layer. You should not
set <code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code> to 1 as this would result in all kernel weights being 0, i.e. the model is not able
to learn.</p></li>
</ul>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="toggle docutils container">
<div class="header docutils container">
<p>The above configuration parameters are the ones you should configure to fit your model to your data.
However, additional parameters exist that can be adapted.</p>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>+---------------------------------+------------------+--------------------------------------------------------------+
| Parameter                       | Default Value    | Description                                                  |
+=================================+==================+==============================================================+
| hidden_layers_sizes             | text: [256, 128] | Hidden layer sizes for layers before the embedding layers    |
|                                 | label: []        | for user messages and labels. The number of hidden layers is |
|                                 |                  | equal to the length of the corresponding.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| share_hidden_layers             | False            | Whether to share the hidden layer weights between user       |
|                                 |                  | messages and labels.                                         |
+---------------------------------+------------------+--------------------------------------------------------------+
| batch_size                      | [64, 256]        | Initial and final value for batch sizes.                     |
|                                 |                  | Batch size will be linearly increased for each epoch.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| batch_strategy                  | &quot;balanced&quot;       | Strategy used when creating batches.                         |
|                                 |                  | Can be either &#39;sequence&#39; or &#39;balanced&#39;.                      |
+---------------------------------+------------------+--------------------------------------------------------------+
| epochs                          | 300              | Number of epochs to train.                                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| random_seed                     | None             | Set random seed to any &#39;int&#39; to get reproducible results.    |
+---------------------------------+------------------+--------------------------------------------------------------+
| learning_rate                   | 0.001            | Initial learning rate for the optimizer.                     |
+---------------------------------+------------------+--------------------------------------------------------------+
| embedding_dimension             | 20               | Dimension size of embedding vectors.                         |
+---------------------------------+------------------+--------------------------------------------------------------+
| dense_dimension                 | text: 512        | Dense dimension for sparse features to use if no dense       |
|                                 | label: 20        | features are present.                                        |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_negative_examples     | 20               | The number of incorrect labels. The algorithm will minimize  |
|                                 |                  | their similarity to the user input during training.          |
+---------------------------------+------------------+--------------------------------------------------------------+
| similarity_type                 | &quot;auto&quot;           | Type of similarity measure to use, either &#39;auto&#39; or &#39;cosine&#39; |
|                                 |                  | or &#39;inner&#39;.                                                  |
+---------------------------------+------------------+--------------------------------------------------------------+
| loss_type                       | &quot;softmax&quot;        | The type of the loss function, either &#39;softmax&#39; or &#39;margin&#39;. |
+---------------------------------+------------------+--------------------------------------------------------------+
| ranking_length                  | 10               | Number of top actions to normalize scores for loss type      |
|                                 |                  | &#39;softmax&#39;. Set to 0 to turn off normalization.               |
+---------------------------------+------------------+--------------------------------------------------------------+
| maximum_positive_similarity     | 0.8              | Indicates how similar the algorithm should try to make       |
|                                 |                  | embedding vectors for correct labels.                        |
|                                 |                  | Should be 0.0 &lt; ... &lt; 1.0 for &#39;cosine&#39; similarity type.      |
+---------------------------------+------------------+--------------------------------------------------------------+
| maximum_negative_similarity     | -0.4             | Maximum negative similarity for incorrect labels.            |
|                                 |                  | Should be -1.0 &lt; ... &lt; 1.0 for &#39;cosine&#39; similarity type.     |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_maximum_negative_similarity | True             | If &#39;True&#39; the algorithm only minimizes maximum similarity    |
|                                 |                  | over incorrect intent labels, used only if &#39;loss_type&#39; is    |
|                                 |                  | set to &#39;margin&#39;.                                             |
+---------------------------------+------------------+--------------------------------------------------------------+
| scale_loss                      | True             | Scale loss inverse proportionally to confidence of correct   |
|                                 |                  | prediction.                                                  |
+---------------------------------+------------------+--------------------------------------------------------------+
| regularization_constant         | 0.002            | The scale of regularization.                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
| negative_margin_scale           | 0.8              | The scale of how important is to minimize the maximum        |
|                                 |                  | similarity between embeddings of different labels.           |
+---------------------------------+------------------+--------------------------------------------------------------+
| weight_sparsity                 | 0.0              | Sparsity of the weights in dense layers.                     |
|                                 |                  | Value should be between 0 and 1.                             |
+---------------------------------+------------------+--------------------------------------------------------------+
| drop_rate                       | 0.2              | Dropout rate for encoder. Value should be between 0 and 1.   |
|                                 |                  | The higher the value the higher the regularization effect.   |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_sparse_input_dropout        | True             | If &#39;True&#39; apply dropout to sparse tensors.                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| evaluate_every_number_of_epochs | 20               | How often to calculate validation accuracy.                  |
|                                 |                  | Set to &#39;-1&#39; to evaluate just once at the end of training.    |
+---------------------------------+------------------+--------------------------------------------------------------+
| evaluate_on_number_of_examples  | 0                | How many examples to use for hold out validation set.        |
|                                 |                  | Large values may hurt performance, e.g. model accuracy.      |
+---------------------------------+------------------+--------------------------------------------------------------+
| tensorboard_log_directory       | None             | If you want to use tensorboard to visualize training         |
|                                 |                  | metrics, set this option to a valid output directory. You    |
|                                 |                  | can view the training metrics after training in tensorboard  |
|                                 |                  | via &#39;tensorboard --logdir &lt;path-to-given-directory&gt;&#39;.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| tensorboard_log_level           | &quot;epoch&quot;          | Define when training metrics for tensorboard should be       |
|                                 |                  | logged. Either after every epoch (&quot;epoch&quot;) or for every      |
|                                 |                  | training step (&quot;minibatch&quot;).                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For <code class="docutils literal notranslate"><span class="pre">cosine</span></code> similarity <code class="docutils literal notranslate"><span class="pre">maximum_positive_similarity</span></code> and <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span></code> should
be between <code class="docutils literal notranslate"><span class="pre">-1</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is an option to use linearly increasing batch size. The idea comes from
<a class="reference external" href="https://arxiv.org/abs/1711.00489">https://arxiv.org/abs/1711.00489</a>.
In order to do it pass a list to <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">[64,</span> <span class="pre">256]</span></code> (default behaviour).
If constant <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is required, pass an <code class="docutils literal notranslate"><span class="pre">int</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">64</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameter <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span></code> is set to a negative value to mimic the original
starspace algorithm in the case <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span> <span class="pre">=</span> <span class="pre">maximum_positive_similarity</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_maximum_negative_similarity</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference external" href="https://arxiv.org/abs/1709.03856">starspace paper</a> for details.</p>
</div>
</div>
</dd>
</dl>
</div>
<div class="section" id="keywordintentclassifier">
<span id="keyword-intent-classifier"></span><h3><a class="toc-backref" href="#id49">KeywordIntentClassifier</a><a class="headerlink" href="#keywordintentclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Simple keyword matching intent classifier, intended for small, short-term projects.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">intent</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This classifier works by searching a message for keywords.
The matching is case sensitive by default and searches only for exact matches of the keyword-string in the user
message.
The keywords for an intent are the examples of that intent in the NLU training data.
This means the entire example is the keyword, not the individual words in the example.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This classifier is intended only for small projects or to get started. If
you have few NLU training data, you can take a look at the recommended pipelines in
<a class="reference internal" href="../choosing-a-pipeline/#choosing-a-pipeline"><span class="std std-ref">Choosing a Pipeline</span></a>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;KeywordIntentClassifier&quot;</span>
  <span class="nt">case_sensitive</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="dietclassifier">
<h3><a class="toc-backref" href="#id50">DIETClassifier</a><a class="headerlink" href="#dietclassifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list simple">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Dual Intent Entity Transformer (DIET) used for intent classification and entity extraction</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>You can find the detailed description of the <a class="reference internal" href="#diet-classifier"><span class="std std-ref">DIETClassifier</span></a> under the section
<cite>Combined Entity Extractors and Intent Classifiers</cite>.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="entity-extractors">
<h2><a class="toc-backref" href="#id51">Entity Extractors</a><a class="headerlink" href="#entity-extractors" title="Permalink to this headline">¶</a></h2>
<p>Entity extractors extract entities, such as person names or locations, from the user message.</p>
<div class="section" id="mitieentityextractor">
<h3><a class="toc-backref" href="#id52">MitieEntityExtractor</a><a class="headerlink" href="#mitieentityextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>MITIE entity extraction (using a <a class="reference external" href="https://github.com/mit-nlp/MITIE/blob/master/mitielib/src/ner_trainer.cpp">MITIE NER trainer</a>)</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mitienlp"><span class="std std-ref">MitieNLP</span></a> and <code class="docutils literal notranslate"><span class="pre">tokens</span></code></p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;New York City&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
        <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
        <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;MitieEntityExtractor&quot;</span>
    <span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">MitieEntityExtractor</span></code> uses the MITIE entity extraction to find entities in a message. The underlying classifier
is using a multi class linear SVM with a sparse linear kernel and custom features.
The MITIE component does not provide entity confidence values.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This entity extractor does not rely on any featurizer as it extracts features on its own.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;MitieEntityExtractor&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="spacyentityextractor">
<span id="id17"></span><h3><a class="toc-backref" href="#id53">SpacyEntityExtractor</a><a class="headerlink" href="#spacyentityextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>spaCy entity extraction</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><a class="reference internal" href="#spacynlp"><span class="std std-ref">SpacyNLP</span></a></p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;New York City&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
        <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
        <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;SpacyEntityExtractor&quot;</span>
    <span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Using spaCy this component predicts the entities of a message. spaCy uses a statistical BILOU transition model.
As of now, this component can only use the spaCy builtin entity extraction models and can not be retrained.
This extractor does not provide any confidence scores.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>Configure which dimensions, i.e. entity types, the spaCy component
should extract. A full list of available dimensions can be found in
the <a class="reference external" href="https://spacy.io/api/annotation#section-named-entities">spaCy documentation</a>.
Leaving the dimensions option unspecified will extract all available dimensions.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;SpacyEntityExtractor&quot;</span>
  <span class="c1"># dimensions to extract</span>
  <span class="nt">dimensions</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&quot;PERSON&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;LOC&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;ORG&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;PRODUCT&quot;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="entitysynonymmapper">
<h3><a class="toc-backref" href="#id54">EntitySynonymMapper</a><a class="headerlink" href="#entitysynonymmapper" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Maps synonymous entity values to the same value.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>Modifies existing entities that previous entity extraction components found.</p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>If the training data contains defined synonyms, this component will make sure that detected entity values will
be mapped to the same value. For example, if your training data contains the following examples:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="p">{</span>
      <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;I moved to New York City&quot;</span><span class="p">,</span>
      <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;inform_relocation&quot;</span><span class="p">,</span>
      <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;nyc&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
      <span class="p">}]</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;I got a new flat in NYC.&quot;</span><span class="p">,</span>
      <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="s2">&quot;inform_relocation&quot;</span><span class="p">,</span>
      <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;nyc&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">23</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
      <span class="p">}]</span>
    <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>This component will allow you to map the entities <code class="docutils literal notranslate"><span class="pre">New</span> <span class="pre">York</span> <span class="pre">City</span></code> and <code class="docutils literal notranslate"><span class="pre">NYC</span></code> to <code class="docutils literal notranslate"><span class="pre">nyc</span></code>. The entity
extraction will return <code class="docutils literal notranslate"><span class="pre">nyc</span></code> even though the message contains <code class="docutils literal notranslate"><span class="pre">NYC</span></code>. When this component changes an
existing entity, it appends itself to the processor list of this entity.</p>
</dd>
<dt class="field-odd">Configuration</dt>
<dd class="field-odd"><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;EntitySynonymMapper&quot;</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="crfentityextractor">
<span id="id18"></span><h3><a class="toc-backref" href="#id55">CRFEntityExtractor</a><a class="headerlink" href="#crfentityextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Conditional random field (CRF) entity extraction</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">dense_features</span></code> (optional)</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;New York City&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">33</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;city&quot;</span><span class="p">,</span>
        <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.874</span><span class="p">,</span>
        <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;CRFEntityExtractor&quot;</span>
    <span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>This component implements a conditional random fields (CRF) to do named entity recognition.
CRFs can be thought of as an undirected Markov chain where the time steps are words
and the states are entity classes. Features of the words (capitalisation, POS tagging,
etc.) give probabilities to certain entity classes, as are transitions between
neighbouring entity tags: the most likely set of tags is then calculated and returned.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">CRFEntityExtractor</span></code> has a list of default features to use.
However, you can overwrite the default configuration.
The following features are available:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>==============  ==========================================================================================
Feature Name    Description
==============  ==========================================================================================
low             Checks if the token is lower case.
upper           Checks if the token is upper case.
title           Checks if the token starts with an uppercase character and all remaining characters are
                lowercased.
digit           Checks if the token contains just digits.
prefix5         Take the first five characters of the token.
prefix2         Take the first two characters of the token.
suffix5         Take the last five characters of the token.
suffix3         Take the last three characters of the token.
suffix2         Take the last two characters of the token.
suffix1         Take the last character of the token.
pos             Take the Part-of-Speech tag of the token (``SpacyTokenizer`` required).
pos2            Take the first two characters of the Part-of-Speech tag of the token
                (``SpacyTokenizer`` required).
pattern         Take the patterns defined by ``RegexFeaturizer``.
bias            Add an additional &quot;bias&quot; feature to the list of features.
==============  ==========================================================================================
</pre></div>
</div>
<p>As the featurizer is moving over the tokens in a user message with a sliding window, you can define features for
previous tokens, the current token, and the next tokens in the sliding window.
You define the features as [before, token, after] array.</p>
<p>Additional you can set a flag to determine whether to use the BILOU tagging schema or not.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BILOU_flag</span></code> determines whether to use BILOU tagging or not. Default <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</div></blockquote>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;CRFEntityExtractor&quot;</span>
  <span class="c1"># BILOU_flag determines whether to use BILOU tagging or not.</span>
  <span class="s">&quot;BILOU_flag&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>
  <span class="c1"># features to extract in the sliding window</span>
  <span class="s">&quot;features&quot;</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span>
    <span class="p p-Indicator">[</span><span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;title&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;upper&quot;</span><span class="p p-Indicator">],</span>
    <span class="p p-Indicator">[</span>
      <span class="s">&quot;bias&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;prefix5&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;prefix2&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;suffix5&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;suffix3&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;suffix2&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;upper&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;title&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;digit&quot;</span><span class="p p-Indicator">,</span>
      <span class="s">&quot;pattern&quot;</span><span class="p p-Indicator">,</span>
    <span class="p p-Indicator">],</span>
    <span class="p p-Indicator">[</span><span class="s">&quot;low&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;title&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;upper&quot;</span><span class="p p-Indicator">],</span>
  <span class="p p-Indicator">]</span>
  <span class="c1"># The maximum number of iterations for optimization algorithms.</span>
  <span class="s">&quot;max_iterations&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">50</span>
  <span class="c1"># weight of the L1 regularization</span>
  <span class="s">&quot;L1_c&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
  <span class="c1"># weight of the L2 regularization</span>
  <span class="s">&quot;L2_c&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If POS features are used (<code class="docutils literal notranslate"><span class="pre">pos</span></code> or <code class="docutils literal notranslate"><span class="pre">pos2`),</span> <span class="pre">you</span> <span class="pre">need</span> <span class="pre">to</span> <span class="pre">have</span> <span class="pre">``SpacyTokenizer</span></code> in your pipeline.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If “<code class="docutils literal notranslate"><span class="pre">pattern`</span> <span class="pre">features</span> <span class="pre">are</span> <span class="pre">used,</span> <span class="pre">you</span> <span class="pre">need</span> <span class="pre">to</span> <span class="pre">have</span> <span class="pre">``RegexFeaturizer</span></code> in your pipeline.</p>
</div>
</dd>
</dl>
</div>
<div class="section" id="ducklinghttpextractor">
<span id="id19"></span><h3><a class="toc-backref" href="#id56">DucklingHTTPExtractor</a><a class="headerlink" href="#ducklinghttpextractor" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Duckling lets you extract common entities like dates,
amounts of money, distances, and others in a number of languages.</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">entities</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p>Nothing</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">53</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;time&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;2017-04-10T00:00:00.000+02:00&quot;</span><span class="p">,</span>
        <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;DucklingHTTPExtractor&quot;</span>
    <span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>To use this component you need to run a duckling server. The easiest
option is to spin up a docker container using
<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">-p</span> <span class="pre">8000:8000</span> <span class="pre">rasa/duckling</span></code>.</p>
<p>Alternatively, you can <a class="reference external" href="https://github.com/facebook/duckling#quickstart">install duckling directly on your
machine</a> and start the server.</p>
<p>Duckling allows to recognize dates, numbers, distances and other structured entities
and normalizes them.
Please be aware that duckling tries to extract as many entity types as possible without
providing a ranking. For example, if you specify both <code class="docutils literal notranslate"><span class="pre">number</span></code> and <code class="docutils literal notranslate"><span class="pre">time</span></code> as dimensions
for the duckling component, the component will extract two entities: <code class="docutils literal notranslate"><span class="pre">10</span></code> as a number and
<code class="docutils literal notranslate"><span class="pre">in</span> <span class="pre">10</span> <span class="pre">minutes</span></code> as a time from the text <code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">will</span> <span class="pre">be</span> <span class="pre">there</span> <span class="pre">in</span> <span class="pre">10</span> <span class="pre">minutes</span></code>. In such a
situation, your application would have to decide which entity type is be the correct one.
The extractor will always return <cite>1.0</cite> as a confidence, as it is a rule
based system.</p>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>Configure which dimensions, i.e. entity types, the duckling component
should extract. A full list of available dimensions can be found in
the <a class="reference external" href="https://duckling.wit.ai/">duckling documentation</a>.
Leaving the dimensions option unspecified will extract all available dimensions.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">pipeline</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="s">&quot;DucklingHTTPExtractor&quot;</span>
  <span class="c1"># url of the running duckling server</span>
  <span class="nt">url</span><span class="p">:</span> <span class="s">&quot;http://localhost:8000&quot;</span>
  <span class="c1"># dimensions to extract</span>
  <span class="nt">dimensions</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="s">&quot;time&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;number&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;amount-of-money&quot;</span><span class="p p-Indicator">,</span> <span class="s">&quot;distance&quot;</span><span class="p p-Indicator">]</span>
  <span class="c1"># allows you to configure the locale, by default the language is</span>
  <span class="c1"># used</span>
  <span class="nt">locale</span><span class="p">:</span> <span class="s">&quot;de_DE&quot;</span>
  <span class="c1"># if not set the default timezone of Duckling is going to be used</span>
  <span class="c1"># needed to calculate dates from relative expressions like &quot;tomorrow&quot;</span>
  <span class="nt">timezone</span><span class="p">:</span> <span class="s">&quot;Europe/Berlin&quot;</span>
  <span class="c1"># Timeout for receiving response from http url of the running duckling server</span>
  <span class="c1"># if not set the default timeout of duckling http url is set to 3 seconds.</span>
  <span class="nt">timeout </span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="id20">
<h3><a class="toc-backref" href="#id57">DIETClassifier</a><a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<dl class="field-list simple">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Dual Intent Entity Transformer (DIET) used for intent classification and entity extraction</p>
</dd>
<dt class="field-even">Description</dt>
<dd class="field-even"><p>You can find the detailed description of the <a class="reference internal" href="#diet-classifier"><span class="std std-ref">DIETClassifier</span></a> under the section
<cite>Combined Entity Extractors and Intent Classifiers</cite>.</p>
</dd>
</dl>
</div>
</div>
<div class="section" id="selectors">
<h2><a class="toc-backref" href="#id58">Selectors</a><a class="headerlink" href="#selectors" title="Permalink to this headline">¶</a></h2>
<p>Selectors predict a bot response from a set of candidate responses.</p>
<div class="section" id="responseselector">
<span id="response-selector"></span><h3><a class="toc-backref" href="#id59">ResponseSelector</a><a class="headerlink" href="#responseselector" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Response Selector</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p>A dictionary with key as <code class="docutils literal notranslate"><span class="pre">direct_response_intent</span></code> and value containing <code class="docutils literal notranslate"><span class="pre">response</span></code> and <code class="docutils literal notranslate"><span class="pre">ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> and/or <code class="docutils literal notranslate"><span class="pre">sparse_features</span></code> for user messages and response</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;response_selector&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="nt">&quot;faq&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;response&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.7356462617</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Supports 3.5, 3.6 and 3.7, recommended version is 3.6&quot;</span><span class="p">},</span>
        <span class="nt">&quot;ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.7356462617</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Supports 3.5, 3.6 and 3.7, recommended version is 3.6&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.2134543431</span><span class="p">,</span> <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;You can ask me about how to get started&quot;</span><span class="p">}</span>
        <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>Response Selector component can be used to build a response retrieval model to directly predict a bot response from
a set of candidate responses. The prediction of this model is used by <a class="reference internal" href="../../core/retrieval-actions/#retrieval-actions"><span class="std std-ref">Retrieval Actions</span></a>.
It embeds user inputs and response labels into the same space and follows the exact same
neural network architecture and optimization as the <a class="reference internal" href="#diet-classifier"><span class="std std-ref">DIETClassifier</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If during prediction time a message contains <strong>only</strong> words unseen during training
and no Out-Of-Vocabulary preprocessor was used, an empty response <code class="docutils literal notranslate"><span class="pre">None</span></code> is predicted with confidence
<code class="docutils literal notranslate"><span class="pre">0.0</span></code>. This might happen if you only use the <a class="reference internal" href="#countvectorsfeaturizer"><span class="std std-ref">CountVectorsFeaturizer</span></a> with a <code class="docutils literal notranslate"><span class="pre">word</span></code> analyzer
as featurizer. If you use the <code class="docutils literal notranslate"><span class="pre">char_wb</span></code> analyzer, you should always get a response with a confidence
value <code class="docutils literal notranslate"><span class="pre">&gt;</span> <span class="pre">0.0</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>The algorithm includes almost all the hyperparameters that <a class="reference internal" href="#diet-classifier"><span class="std std-ref">DIETClassifier</span></a> uses.
If you want to adapt your model, start by modifying the following parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code>:
This parameter sets the number of times the algorithm will see the training data (default: <code class="docutils literal notranslate"><span class="pre">300</span></code>).
One <code class="docutils literal notranslate"><span class="pre">epoch</span></code> is equals to one forward pass and one backward pass of all the training examples.
Sometimes the model needs more epochs to properly learn.
Sometimes more epochs don’t influence the performance.
The lower the number of epochs the faster the model is trained.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes</span></code>:
This parameter allows you to define the number of feed forward layers and their output
dimensions for user messages and intents (default: <code class="docutils literal notranslate"><span class="pre">text:</span> <span class="pre">[256,</span> <span class="pre">128],</span> <span class="pre">label:</span> <span class="pre">[256,</span> <span class="pre">128]</span></code>).
Every entry in the list corresponds to a feed forward layer.
For example, if you set <code class="docutils literal notranslate"><span class="pre">text:</span> <span class="pre">[256,</span> <span class="pre">128]</span></code>, we will add two feed forward layers in front of
the transformer. The vectors of the input tokens (coming from the user message) will be passed on to those
layers. The first layer will have an output dimension of 256 and the second layer will have an output
dimension of 128. If an empty list is used (default behaviour), no feed forward layer will be
added.
Make sure to use only positive integer values. Usually, numbers of power of two are used.
Also, it is usual practice to have decreasing values in the list: next value is smaller or equal to the
value before.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_dimension</span></code>:
This parameter defines the output dimension of the embedding layers used inside the model (default: <code class="docutils literal notranslate"><span class="pre">20</span></code>).
We are using multiple embeddings layers inside the model architecture.
For example, the vector of the <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> token and the intent is passed on to an embedding layer before
they are compared and the loss is calculated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_transformer_layers</span></code>:
This parameter sets the number of transformer layers to use (default: <code class="docutils literal notranslate"><span class="pre">0</span></code>).
The number of transformer layers corresponds to the transformer blocks to use for the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transformer_size</span></code>:
This parameter sets the number of units in the transformer (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>).
The vectors coming out of the transformers will have the given <code class="docutils literal notranslate"><span class="pre">transformer_size</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code>:
This parameter defines the fraction of kernel weights that are set to 0 for all feed forward layers
in the model (default: <code class="docutils literal notranslate"><span class="pre">0.8</span></code>). The value should be between 0 and 1. If you set <code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code>
to 0, no kernel weights will be set to 0, the layer acts as a standard feed forward layer. You should not
set <code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code> to 1 as this would result in all kernel weights being 0, i.e. the model is not able
to learn.</p></li>
</ul>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>In addition, the component can also be configured to train a response selector for a particular retrieval intent.
The parameter <code class="docutils literal notranslate"><span class="pre">retrieval_intent</span></code> sets the name of the intent for which this response selector model is trained.
Default is <code class="docutils literal notranslate"><span class="pre">None</span></code>, i.e. the model is trained for all retrieval intents.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="toggle docutils container">
<div class="header docutils container">
<p>The above configuration parameters are the ones you should configure to fit your model to your data.
However, additional parameters exist that can be adapted.</p>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>+---------------------------------+-------------------+--------------------------------------------------------------+
| Parameter                       | Default Value     | Description                                                  |
+=================================+===================+==============================================================+
| hidden_layers_sizes             | text: [256, 128]  | Hidden layer sizes for layers before the embedding layers    |
|                                 | label: [256, 128] | for user messages and labels. The number of hidden layers is |
|                                 |                   | equal to the length of the corresponding.                    |
+---------------------------------+-------------------+--------------------------------------------------------------+
| share_hidden_layers             | False             | Whether to share the hidden layer weights between user       |
|                                 |                   | messages and labels.                                         |
+---------------------------------+-------------------+--------------------------------------------------------------+
| transformer_size                | None              | Number of units in transformer.                              |
+---------------------------------+-------------------+--------------------------------------------------------------+
| number_of_transformer_layers    | 0                 | Number of transformer layers.                                |
+---------------------------------+-------------------+--------------------------------------------------------------+
| number_of_attention_heads       | 4                 | Number of attention heads in transformer.                    |
+---------------------------------+-------------------+--------------------------------------------------------------+
| use_key_relative_attention      | False             | If &#39;True&#39; use key relative embeddings in attention.          |
+---------------------------------+-------------------+--------------------------------------------------------------+
| use_value_relative_attention    | False             | If &#39;True&#39; use value relative embeddings in attention.        |
+---------------------------------+-------------------+--------------------------------------------------------------+
| max_relative_position           | None              | Maximum position for relative embeddings.                    |
+---------------------------------+-------------------+--------------------------------------------------------------+
| unidirectional_encoder          | False             | Use a unidirectional or bidirectional encoder.               |
+---------------------------------+-------------------+--------------------------------------------------------------+
| batch_size                      | [64, 256]         | Initial and final value for batch sizes.                     |
|                                 |                   | Batch size will be linearly increased for each epoch.        |
+---------------------------------+-------------------+--------------------------------------------------------------+
| batch_strategy                  | &quot;balanced&quot;        | Strategy used when creating batches.                         |
|                                 |                   | Can be either &#39;sequence&#39; or &#39;balanced&#39;.                      |
+---------------------------------+-------------------+--------------------------------------------------------------+
| epochs                          | 300               | Number of epochs to train.                                   |
+---------------------------------+-------------------+--------------------------------------------------------------+
| random_seed                     | None              | Set random seed to any &#39;int&#39; to get reproducible results.    |
+---------------------------------+-------------------+--------------------------------------------------------------+
| learning_rate                   | 0.001             | Initial learning rate for the optimizer.                     |
+---------------------------------+-------------------+--------------------------------------------------------------+
| embedding_dimension             | 20                | Dimension size of embedding vectors.                         |
+---------------------------------+-------------------+--------------------------------------------------------------+
| dense_dimension                 | text: 512         | Dense dimension for sparse features to use if no dense       |
|                                 | label: 512        | features are present.                                        |
+---------------------------------+-------------------+--------------------------------------------------------------+
| number_of_negative_examples     | 20                | The number of incorrect labels. The algorithm will minimize  |
|                                 |                   | their similarity to the user input during training.          |
+---------------------------------+-------------------+--------------------------------------------------------------+
| similarity_type                 | &quot;auto&quot;            | Type of similarity measure to use, either &#39;auto&#39; or &#39;cosine&#39; |
|                                 |                   | or &#39;inner&#39;.                                                  |
+---------------------------------+-------------------+--------------------------------------------------------------+
| loss_type                       | &quot;softmax&quot;         | The type of the loss function, either &#39;softmax&#39; or &#39;margin&#39;. |
+---------------------------------+-------------------+--------------------------------------------------------------+
| ranking_length                  | 10                | Number of top actions to normalize scores for loss type      |
|                                 |                   | &#39;softmax&#39;. Set to 0 to turn off normalization.               |
+---------------------------------+-------------------+--------------------------------------------------------------+
| maximum_positive_similarity     | 0.8               | Indicates how similar the algorithm should try to make       |
|                                 |                   | embedding vectors for correct labels.                        |
|                                 |                   | Should be 0.0 &lt; ... &lt; 1.0 for &#39;cosine&#39; similarity type.      |
+---------------------------------+-------------------+--------------------------------------------------------------+
| maximum_negative_similarity     | -0.4              | Maximum negative similarity for incorrect labels.            |
|                                 |                   | Should be -1.0 &lt; ... &lt; 1.0 for &#39;cosine&#39; similarity type.     |
+---------------------------------+-------------------+--------------------------------------------------------------+
| use_maximum_negative_similarity | True              | If &#39;True&#39; the algorithm only minimizes maximum similarity    |
|                                 |                   | over incorrect intent labels, used only if &#39;loss_type&#39; is    |
|                                 |                   | set to &#39;margin&#39;.                                             |
+---------------------------------+-------------------+--------------------------------------------------------------+
| scale_loss                      | True              | Scale loss inverse proportionally to confidence of correct   |
|                                 |                   | prediction.                                                  |
+---------------------------------+-------------------+--------------------------------------------------------------+
| regularization_constant         | 0.002             | The scale of regularization.                                 |
+---------------------------------+-------------------+--------------------------------------------------------------+
| negative_margin_scale           | 0.8               | The scale of how important is to minimize the maximum        |
|                                 |                   | similarity between embeddings of different labels.           |
+---------------------------------+-------------------+--------------------------------------------------------------+
| weight_sparsity                 | 0.8               | Sparsity of the weights in dense layers.                     |
|                                 |                   | Value should be between 0 and 1.                             |
+---------------------------------+-------------------+--------------------------------------------------------------+
| drop_rate                       | 0.2               | Dropout rate for encoder. Value should be between 0 and 1.   |
|                                 |                   | The higher the value the higher the regularization effect.   |
+---------------------------------+-------------------+--------------------------------------------------------------+
| drop_rate_attention             | 0.0               | Dropout rate for attention. Value should be between 0 and 1. |
|                                 |                   | The higher the value the higher the regularization effect.   |
+---------------------------------+-------------------+--------------------------------------------------------------+
| use_sparse_input_dropout        | False             | If &#39;True&#39; apply dropout to sparse tensors.                   |
+---------------------------------+-------------------+--------------------------------------------------------------+
| evaluate_every_number_of_epochs | 20                | How often to calculate validation accuracy.                  |
|                                 |                   | Set to &#39;-1&#39; to evaluate just once at the end of training.    |
+---------------------------------+-------------------+--------------------------------------------------------------+
| evaluate_on_number_of_examples  | 0                 | How many examples to use for hold out validation set.        |
|                                 |                   | Large values may hurt performance, e.g. model accuracy.      |
+---------------------------------+-------------------+--------------------------------------------------------------+
| use_masked_language_model       | False             | If &#39;True&#39; random tokens of the input message will be masked  |
|                                 |                   | and the model should predict those tokens.                   |
+---------------------------------+-------------------+--------------------------------------------------------------+
| retrieval_intent                | None              | Name of the intent for which this response selector model is |
|                                 |                   | trained.                                                     |
+---------------------------------+-------------------+--------------------------------------------------------------+
| tensorboard_log_directory       | None              | If you want to use tensorboard to visualize training         |
|                                 |                   | metrics, set this option to a valid output directory. You    |
|                                 |                   | can view the training metrics after training in tensorboard  |
|                                 |                   | via &#39;tensorboard --logdir &lt;path-to-given-directory&gt;&#39;.        |
+---------------------------------+-------------------+--------------------------------------------------------------+
| tensorboard_log_level           | &quot;epoch&quot;           | Define when training metrics for tensorboard should be       |
|                                 |                   | logged. Either after every epoch (&quot;epoch&quot;) or for every      |
|                                 |                   | training step (&quot;minibatch&quot;).                                 |
+---------------------------------+-------------------+--------------------------------------------------------------+
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For <code class="docutils literal notranslate"><span class="pre">cosine</span></code> similarity <code class="docutils literal notranslate"><span class="pre">maximum_positive_similarity</span></code> and <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span></code> should
be between <code class="docutils literal notranslate"><span class="pre">-1</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is an option to use linearly increasing batch size. The idea comes from
<a class="reference external" href="https://arxiv.org/abs/1711.00489">https://arxiv.org/abs/1711.00489</a>.
In order to do it pass a list to <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">[64,</span> <span class="pre">256]</span></code> (default behaviour).
If constant <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is required, pass an <code class="docutils literal notranslate"><span class="pre">int</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">64</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameter <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span></code> is set to a negative value to mimic the original
starspace algorithm in the case <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span> <span class="pre">=</span> <span class="pre">maximum_positive_similarity</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_maximum_negative_similarity</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference external" href="https://arxiv.org/abs/1709.03856">starspace paper</a> for details.</p>
</div>
</div>
</dd>
</dl>
</div>
</div>
<div class="section" id="combined-entity-extractors-and-intent-classifiers">
<h2><a class="toc-backref" href="#id60">Combined Entity Extractors and Intent Classifiers</a><a class="headerlink" href="#combined-entity-extractors-and-intent-classifiers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="diet-classifier">
<span id="id23"></span><h3><a class="toc-backref" href="#id61">DIETClassifier</a><a class="headerlink" href="#diet-classifier" title="Permalink to this headline">¶</a></h3>
<dl class="field-list">
<dt class="field-odd">Short</dt>
<dd class="field-odd"><p>Dual Intent Entity Transformer (DIET) used for intent classification and entity extraction</p>
</dd>
<dt class="field-even">Outputs</dt>
<dd class="field-even"><p><code class="docutils literal notranslate"><span class="pre">entities</span></code>, <code class="docutils literal notranslate"><span class="pre">intent</span></code> and <code class="docutils literal notranslate"><span class="pre">intent_ranking</span></code></p>
</dd>
<dt class="field-odd">Requires</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">dense_features</span></code> and/or <code class="docutils literal notranslate"><span class="pre">sparse_features</span></code> for user message and optionally the intent</p>
</dd>
<dt class="field-even">Output-Example</dt>
<dd class="field-even"><div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;intent&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;greet&quot;</span><span class="p">,</span> <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.8343</span><span class="p">},</span>
    <span class="nt">&quot;intent_ranking&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.385910906220309</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;goodbye&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">0.28161531595656784</span><span class="p">,</span>
            <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;restaurant_search&quot;</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="nt">&quot;entities&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="nt">&quot;end&quot;</span><span class="p">:</span> <span class="mi">53</span><span class="p">,</span>
        <span class="nt">&quot;entity&quot;</span><span class="p">:</span> <span class="s2">&quot;time&quot;</span><span class="p">,</span>
        <span class="nt">&quot;start&quot;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span>
        <span class="nt">&quot;value&quot;</span><span class="p">:</span> <span class="s2">&quot;2017-04-10T00:00:00.000+02:00&quot;</span><span class="p">,</span>
        <span class="nt">&quot;confidence&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="nt">&quot;extractor&quot;</span><span class="p">:</span> <span class="s2">&quot;DIETClassifier&quot;</span>
    <span class="p">}]</span>
<span class="p">}</span>
</pre></div>
</div>
</dd>
<dt class="field-odd">Description</dt>
<dd class="field-odd"><p>DIET (Dual Intent and Entity Transformer) is a multi-task architecture for intent classification and entity
recognition. The architecture is based on a transformer which is shared for both tasks.
A sequence of entity labels is predicted through a Conditional Random Field (CRF) tagging layer on top of the
transformer output sequence corresponding to the input sequence of tokens.
For the intent labels the transformer output for the <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> token and intent labels are embedded into a
single semantic vector space. We use the dot-product loss to maximize the similarity with the target label and
minimize similarities with negative samples.</p>
<p>If you want to learn more about the model, please take a look at our
<a class="reference external" href="https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb">videos</a> where we explain the model
architecture in detail.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If during prediction time a message contains <strong>only</strong> words unseen during training
and no Out-Of-Vocabulary preprocessor was used, an empty intent <code class="docutils literal notranslate"><span class="pre">None</span></code> is predicted with confidence
<code class="docutils literal notranslate"><span class="pre">0.0</span></code>. This might happen if you only use the <a class="reference internal" href="#countvectorsfeaturizer"><span class="std std-ref">CountVectorsFeaturizer</span></a> with a <code class="docutils literal notranslate"><span class="pre">word</span></code> analyzer
as featurizer. If you use the <code class="docutils literal notranslate"><span class="pre">char_wb</span></code> analyzer, you should always get an intent with a confidence
value <code class="docutils literal notranslate"><span class="pre">&gt;</span> <span class="pre">0.0</span></code>.</p>
</div>
</dd>
<dt class="field-even">Configuration</dt>
<dd class="field-even"><p>If you want to use the <code class="docutils literal notranslate"><span class="pre">DIETClassifier</span></code> just for intent classification, set <code class="docutils literal notranslate"><span class="pre">entity_recognition</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
If you want to do only entity recognition, set <code class="docutils literal notranslate"><span class="pre">intent_classification</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
By default <code class="docutils literal notranslate"><span class="pre">DIETClassifier</span></code> does both, i.e. <code class="docutils literal notranslate"><span class="pre">entity_recognition</span></code> and <code class="docutils literal notranslate"><span class="pre">intent_classification</span></code> are set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>You can define a number of hyperparameters to adapt the model.
If you want to adapt your model, start by modifying the following parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code>:
This parameter sets the number of times the algorithm will see the training data (default: <code class="docutils literal notranslate"><span class="pre">300</span></code>).
One <code class="docutils literal notranslate"><span class="pre">epoch</span></code> is equals to one forward pass and one backward pass of all the training examples.
Sometimes the model needs more epochs to properly learn.
Sometimes more epochs don’t influence the performance.
The lower the number of epochs the faster the model is trained.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_layers_sizes</span></code>:
This parameter allows you to define the number of feed forward layers and their output
dimensions for user messages and intents (default: <code class="docutils literal notranslate"><span class="pre">text:</span> <span class="pre">[],</span> <span class="pre">label:</span> <span class="pre">[]</span></code>).
Every entry in the list corresponds to a feed forward layer.
For example, if you set <code class="docutils literal notranslate"><span class="pre">text:</span> <span class="pre">[256,</span> <span class="pre">128]</span></code>, we will add two feed forward layers in front of
the transformer. The vectors of the input tokens (coming from the user message) will be passed on to those
layers. The first layer will have an output dimension of 256 and the second layer will have an output
dimension of 128. If an empty list is used (default behaviour), no feed forward layer will be
added.
Make sure to use only positive integer values. Usually, numbers of power of two are used.
Also, it is usual practice to have decreasing values in the list: next value is smaller or equal to the
value before.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embedding_dimension</span></code>:
This parameter defines the output dimension of the embedding layers used inside the model (default: <code class="docutils literal notranslate"><span class="pre">20</span></code>).
We are using multiple embeddings layers inside the model architecture.
For example, the vector of the <code class="docutils literal notranslate"><span class="pre">__CLS__</span></code> token and the intent is passed on to an embedding layer before
they are compared and the loss is calculated.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">number_of_transformer_layers</span></code>:
This parameter sets the number of transformer layers to use (default: <code class="docutils literal notranslate"><span class="pre">2</span></code>).
The number of transformer layers corresponds to the transformer blocks to use for the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">transformer_size</span></code>:
This parameter sets the number of units in the transformer (default: <code class="docutils literal notranslate"><span class="pre">256</span></code>).
The vectors coming out of the transformers will have the given <code class="docutils literal notranslate"><span class="pre">transformer_size</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code>:
This parameter defines the fraction of kernel weights that are set to 0 for all feed forward layers
in the model (default: <code class="docutils literal notranslate"><span class="pre">0.8</span></code>). The value should be between 0 and 1. If you set <code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code>
to 0, no kernel weights will be set to 0, the layer acts as a standard feed forward layer. You should not
set <code class="docutils literal notranslate"><span class="pre">weight_sparsity</span></code> to 1 as this would result in all kernel weights being 0, i.e. the model is not able
to learn.</p></li>
</ul>
</div></blockquote>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="toggle docutils container">
<div class="header docutils container">
<p>The above configuration parameters are the ones you should configure to fit your model to your data.
However, additional parameters exist that can be adapted.</p>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>+---------------------------------+------------------+--------------------------------------------------------------+
| Parameter                       | Default Value    | Description                                                  |
+=================================+==================+==============================================================+
| hidden_layers_sizes             | text: []         | Hidden layer sizes for layers before the embedding layers    |
|                                 | label: []        | for user messages and labels. The number of hidden layers is |
|                                 |                  | equal to the length of the corresponding.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| share_hidden_layers             | False            | Whether to share the hidden layer weights between user       |
|                                 |                  | messages and labels.                                         |
+---------------------------------+------------------+--------------------------------------------------------------+
| transformer_size                | 256              | Number of units in transformer.                              |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_transformer_layers    | 2                | Number of transformer layers.                                |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_attention_heads       | 4                | Number of attention heads in transformer.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_key_relative_attention      | False            | If &#39;True&#39; use key relative embeddings in attention.          |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_value_relative_attention    | False            | If &#39;True&#39; use value relative embeddings in attention.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| max_relative_position           | None             | Maximum position for relative embeddings.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| unidirectional_encoder          | False            | Use a unidirectional or bidirectional encoder.               |
+---------------------------------+------------------+--------------------------------------------------------------+
| batch_size                      | [64, 256]        | Initial and final value for batch sizes.                     |
|                                 |                  | Batch size will be linearly increased for each epoch.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| batch_strategy                  | &quot;balanced&quot;       | Strategy used when creating batches.                         |
|                                 |                  | Can be either &#39;sequence&#39; or &#39;balanced&#39;.                      |
+---------------------------------+------------------+--------------------------------------------------------------+
| epochs                          | 300              | Number of epochs to train.                                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| random_seed                     | None             | Set random seed to any &#39;int&#39; to get reproducible results.    |
+---------------------------------+------------------+--------------------------------------------------------------+
| learning_rate                   | 0.001            | Initial learning rate for the optimizer.                     |
+---------------------------------+------------------+--------------------------------------------------------------+
| embedding_dimension             | 20               | Dimension size of embedding vectors.                         |
+---------------------------------+------------------+--------------------------------------------------------------+
| dense_dimension                 | text: 512        | Dense dimension for sparse features to use if no dense       |
|                                 | label: 20        | features are present.                                        |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_negative_examples     | 20               | The number of incorrect labels. The algorithm will minimize  |
|                                 |                  | their similarity to the user input during training.          |
+---------------------------------+------------------+--------------------------------------------------------------+
| similarity_type                 | &quot;auto&quot;           | Type of similarity measure to use, either &#39;auto&#39; or &#39;cosine&#39; |
|                                 |                  | or &#39;inner&#39;.                                                  |
+---------------------------------+------------------+--------------------------------------------------------------+
| loss_type                       | &quot;softmax&quot;        | The type of the loss function, either &#39;softmax&#39; or &#39;margin&#39;. |
+---------------------------------+------------------+--------------------------------------------------------------+
| ranking_length                  | 10               | Number of top actions to normalize scores for loss type      |
|                                 |                  | &#39;softmax&#39;. Set to 0 to turn off normalization.               |
+---------------------------------+------------------+--------------------------------------------------------------+
| maximum_positive_similarity     | 0.8              | Indicates how similar the algorithm should try to make       |
|                                 |                  | embedding vectors for correct labels.                        |
|                                 |                  | Should be 0.0 &lt; ... &lt; 1.0 for &#39;cosine&#39; similarity type.      |
+---------------------------------+------------------+--------------------------------------------------------------+
| maximum_negative_similarity     | -0.4             | Maximum negative similarity for incorrect labels.            |
|                                 |                  | Should be -1.0 &lt; ... &lt; 1.0 for &#39;cosine&#39; similarity type.     |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_maximum_negative_similarity | True             | If &#39;True&#39; the algorithm only minimizes maximum similarity    |
|                                 |                  | over incorrect intent labels, used only if &#39;loss_type&#39; is    |
|                                 |                  | set to &#39;margin&#39;.                                             |
+---------------------------------+------------------+--------------------------------------------------------------+
| scale_loss                      | True             | Scale loss inverse proportionally to confidence of correct   |
|                                 |                  | prediction.                                                  |
+---------------------------------+------------------+--------------------------------------------------------------+
| regularization_constant         | 0.002            | The scale of regularization.                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
| negative_margin_scale           | 0.8              | The scale of how important it is to minimize the maximum     |
|                                 |                  | similarity between embeddings of different labels.           |
+---------------------------------+------------------+--------------------------------------------------------------+
| weight_sparsity                 | 0.8              | Sparsity of the weights in dense layers.                     |
|                                 |                  | Value should be between 0 and 1.                             |
+---------------------------------+------------------+--------------------------------------------------------------+
| drop_rate                       | 0.2              | Dropout rate for encoder. Value should be between 0 and 1.   |
|                                 |                  | The higher the value the higher the regularization effect.   |
+---------------------------------+------------------+--------------------------------------------------------------+
| drop_rate_attention             | 0.0              | Dropout rate for attention. Value should be between 0 and 1. |
|                                 |                  | The higher the value the higher the regularization effect.   |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_sparse_input_dropout        | True             | If &#39;True&#39; apply dropout to sparse tensors.                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| evaluate_every_number_of_epochs | 20               | How often to calculate validation accuracy.                  |
|                                 |                  | Set to &#39;-1&#39; to evaluate just once at the end of training.    |
+---------------------------------+------------------+--------------------------------------------------------------+
| evaluate_on_number_of_examples  | 0                | How many examples to use for hold out validation set.        |
|                                 |                  | Large values may hurt performance, e.g. model accuracy.      |
+---------------------------------+------------------+--------------------------------------------------------------+
| intent_classification           | True             | If &#39;True&#39; intent classification is trained and intents are   |
|                                 |                  | predicted.                                                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| entity_recognition              | True             | If &#39;True&#39; entity recognition is trained and entities are     |
|                                 |                  | extracted.                                                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_masked_language_model       | False            | If &#39;True&#39; random tokens of the input message will be masked  |
|                                 |                  | and the model has to predict those tokens. It acts like a    |
|                                 |                  | regularizer and should help to learn a better contextual     |
|                                 |                  | representation of the input.                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
| tensorboard_log_directory       | None             | If you want to use tensorboard to visualize training         |
|                                 |                  | metrics, set this option to a valid output directory. You    |
|                                 |                  | can view the training metrics after training in tensorboard  |
|                                 |                  | via &#39;tensorboard --logdir &lt;path-to-given-directory&gt;&#39;.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| tensorboard_log_level           | &quot;epoch&quot;          | Define when training metrics for tensorboard should be       |
|                                 |                  | logged. Either after every epoch (&#39;epoch&#39;) or for every      |
|                                 |                  | training step (&#39;minibatch&#39;).                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For <code class="docutils literal notranslate"><span class="pre">cosine</span></code> similarity <code class="docutils literal notranslate"><span class="pre">maximum_positive_similarity</span></code> and <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span></code> should
be between <code class="docutils literal notranslate"><span class="pre">-1</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is an option to use linearly increasing batch size. The idea comes from
<a class="reference external" href="https://arxiv.org/abs/1711.00489">https://arxiv.org/abs/1711.00489</a>.
In order to do it pass a list to <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">[64,</span> <span class="pre">256]</span></code> (default behaviour).
If constant <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is required, pass an <code class="docutils literal notranslate"><span class="pre">int</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">&quot;batch_size&quot;:</span> <span class="pre">64</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameter <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span></code> is set to a negative value to mimic the original
starspace algorithm in the case <code class="docutils literal notranslate"><span class="pre">maximum_negative_similarity</span> <span class="pre">=</span> <span class="pre">maximum_positive_similarity</span></code>
and <code class="docutils literal notranslate"><span class="pre">use_maximum_negative_similarity</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference external" href="https://arxiv.org/abs/1709.03856">starspace paper</a> for details.</p>
</div>
</div>
</dd>
</dl>
</div>
</div>
</div>


          </div>

          <div class="footer">
            <div class="questions">
              Stuck?
              <a class="reference external" href="https://forum.rasa.com" target="_blank">Ask a Question</a>
              or
              <a class="reference external" href="https://github.com/rasahq/rasa/issues" target="_blank">Create an Issue</a>
            </div>
            <div class="social">
              <a href="https://github.com/RasaHQ" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
              <a href="https://stackoverflow.com/search?q=rasa" target="_blank" title="Stack Overflow"><i class="fab fa-stack-overflow"></i></a>
              <a href="https://www.youtube.com/channel/UCJ0V6493mLvqdiVwOKWBODQ" target="_blank" title="YouTube"><i class="fab fa-youtube"></i></a>
              <a href="https://twitter.com/rasa_hq" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
            </div>

            <div class="copyright small">
            &copy;2020, Rasa Technologies | <a href="https://rasa.com/imprint/" target="_blank">Imprint</a> | <a href="https://rasa.com/privacy-policy/" target="_blank">Privacy Policy</a>
            
            </div>

          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>


    

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag(...arguments) {
      dataLayer.push(...arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-87333416-1', {
      'anonymize_ip': true,
    });
  </script>
  <script src="https://rasa.com/assets/js/js.cookies.js"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script type="opt-in" data-name="analytics" data-type="text/javascript" data-src="https://www.googletagmanager.com/gtag/js?id=UA-87333416-1"></script>
  <script type="opt-in" data-name="analytics" data-type="text/javascript" data-src="https://rasa.com/assets/js/userId.js"></script>

  <script type="text/javascript">
    var clipboard = new ClipboardJS('.copyable');
    clipboard.on('success', function(e) {
      gtag('event', e.action, {
        'event_category': 'code',
        'event_label': e.text
      });
      const id = e.text.replace(/ /g,'-');
      document.getElementById(id).classList.add('visible');
      setTimeout(function(){
        document.getElementById(id).classList.remove('visible');},
        800
      );
    });
    clipboard.on('error', function(e) {
      console.log(e);
    });

  </script>

  <!-- ACE Editor, Train & download buttons -->
  <script>
    let updateIntervalId;

    function uuidv4() {
      return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>
              (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)
      );
    }

    function fetchTracker(url, chatBlockId, conversationId) {
      $.ajax({
        url: url + "/conversations/" + conversationId + "/tracker",
        method: "get",
        dataType: 'json',
        contentType: 'application/json',
        success: function(tracker, status) {
          initChatBlock(url, chatBlockId, conversationId, tracker);
        }
      });
    }

    function sendMessage(url, chatBlockId, conversationId, tracker, message) {
      $.ajax({
        url: url + "/webhooks/rest/webhook",
        method: "post",
        dataType: 'json',
        contentType: 'application/json',
        data: JSON.stringify({
          sender: conversationId,
          message: message
        }),
        success: function(result, status) {
          fetchTracker(url, chatBlockId, conversationId);
        },
      });
    }

    function startFetchingTracker(url, chatBlockId, conversationId) {
      const interval = 2000;

      fetchTracker(url, chatBlockId, conversationId);

      updateIntervalId = setInterval(() => {
        fetchTracker(url, chatBlockId, conversationId);
      }, interval);
    }

    function initChatBlock(url, id, conversationId, tracker) {
      ChatBlock.default.init({
        onSendMessage: (message) => {
          sendMessage(url, id, conversationId, tracker, message);
        },
        username: conversationId,
        tracker: tracker,
        selector: id
      });
    }

    const chatBlockId = '#rasa-chat-block';
    const trackingId = uuidv4();

    $(document).ready(() => {
      // make editors work
      const editors = document.querySelectorAll('.ace-editor');

      const assignTrainingDataToButton = function(e, id, trainButton) {
        const trainingData = trainButton.data('training');

        trainButton.data('training', JSON.stringify({
          ...JSON.parse(trainingData || "{}") || {},
          [id]: e.getValue(),
        }));
      };

      ace.config.set('modePath', "/_static/ace/src-min-noconflict");

      Array.from(editors).forEach(function(editor) {
        const e = ace.edit(editor);
        const id = editor.dataset['id'];
        const trackingEndpoint = editor.dataset['tr-endpoint'];

        e.session.setMode("ace/mode/" + editor.dataset['language']);
        e.renderer.setShowGutter(false);
        e.renderer.setShowPrintMargin(false);
        e.renderer.setPadding(24);
        e.renderer.setScrollMargin(24);
        e.setHighlightActiveLine(false);

        if (trackingEndpoint) {
          e.on("change", function() {
            if (!editor.dataset.hasChanged) {
              editor.dataset.hasChanged = true;
              $.ajax({
                url: trackingEndpoint,
                method: "POST",
                data: {editor: id},
              });
            }
          });
        }

        const trainButton = $('.train__button');

        assignTrainingDataToButton(e, id, trainButton);
        e.on("blur", function() {
          assignTrainingDataToButton(e, id, trainButton);
        });

        editor.style.height = editor.dataset['height'] + 'px';
      });

      // initialize a chat block
      initChatBlock("", chatBlockId, trackingId, {});
    });

    // set train button callback
    $( ".train__button" ).click(function() {
      const trainButton = $('.train__button');
      const trainSpinner = $('.train__spinner');
      const downloadButton = $('.download__button');

      trainButton.prop("disabled", true);
      downloadButton.prop("disabled", true);
      trainSpinner.removeClass('train__spinner--hidden');

      if (updateIntervalId) {
        clearInterval(updateIntervalId);
        initChatBlock("", chatBlockId, trackingId, {});
      }

      $.ajax({
        url: trainButton.data('endpoint'),
        method: trainButton.data('method'),
        dataType: 'json',
        contentType: 'application/json',
        data: JSON.stringify({ tracking_id: trackingId, ...JSON.parse(trainButton.data('training')) }),
        success: function(result, status) {
          trainSpinner.addClass('train__spinner--hidden');
          trainButton.prop("disabled", false);

          if (result['project_download_url']) {
            trainSpinner.addClass('train__spinner--hidden');
            downloadButton.prop("disabled", false);
            downloadButton.data("url", result['project_download_url']);
          }

          if (result['rasa_service_url']) {
            const conversationId = uuidv4();
            startFetchingTracker(result['rasa_service_url'], chatBlockId, conversationId);
          }
        },
        error: function(result, status) {
          trainSpinner.addClass('train__spinner--hidden');
          trainButton.prop("disabled", false);
        }
      });
    });

    // set download button callback
    $( ".download__button" ).click(function() {
      const downloadButton = $('.download__button');
      const url = downloadButton.data("url");

      if (url) {
        location.href = url;
      }
    });
  </script>

  <!-- Dismissable announcement banner -->
  <script>
    var banners = document.querySelectorAll('.announcement-banner');

    Array.from(banners).forEach(function(banner) {
      var cookie_id = banner.dataset['cookie-id'];

      if (localStorage.getItem(cookie_id) !== 'true') {
        banner.classList.add('announcement-banner--visible');
      }

      var bannerCloseButton = banner.querySelector('.announcement-banner__close');

      bannerCloseButton && bannerCloseButton.addEventListener('click', function() {
        localStorage.setItem(cookie_id, 'true');
        banner.classList.remove('announcement-banner--visible');
      });
    });
  </script>

  <!-- onsite anchor fix (otherwise anchors scroll to far) -->
  <script>
    /* Adapted from https://stackoverflow.com/a/13067009/1906073 */
    (function(document, history, location) {
      var HISTORY_SUPPORT = !!(history && history.pushState);

      var anchorScrolls = {
        ANCHOR_REGEX: /^#[^ ]+$/,
        OFFSET_HEIGHT_PX: 66,

        /**
         * Establish events, and fix initial scroll position if a hash is provided.
         */
        init: function() {
          this.scrollToCurrent();
          $(window).on('hashchange', $.proxy(this, 'scrollToCurrent'));
          $('body').on('click', 'a', $.proxy(this, 'delegateAnchors'));
        },

        /**
         * Return the offset amount to deduct from the normal scroll position.
         * Modify as appropriate to allow for dynamic calculations
         */
        getFixedOffset: function() {
          return this.OFFSET_HEIGHT_PX;
        },

        /**
         * If the provided href is an anchor which resolves to an element on the
         * page, scroll to it.
         * @param  {String} href
         * @return {Boolean} - Was the href an anchor.
         */
        scrollIfAnchor: function(href, pushToHistory) {
          var match, anchorOffset;

          if(!this.ANCHOR_REGEX.test(href)) {
            return false;
          }

          match = document.getElementById(href.slice(1));

          if(match) {
            anchorOffset = $(match).offset().top - this.getFixedOffset();
            $('html, body').animate({ scrollTop: anchorOffset});

            // Add the state to history as-per normal anchor links
            if(HISTORY_SUPPORT && pushToHistory) {
              history.pushState({}, document.title, location.pathname + href);
            }
          }

          return !!match;
        },

        /**
         * Attempt to scroll to the current location's hash.
         */
        scrollToCurrent: function(e) {
          if(this.scrollIfAnchor(window.location.hash) && e) {
            e.preventDefault();
          }
        },

        /**
         * If the click event's target was an anchor, fix the scroll position.
         */
        delegateAnchors: function(e) {
          var elem = e.target;

          if(this.scrollIfAnchor(elem.getAttribute('href'), true)) {
            e.preventDefault();
          }
        }
      };

      $(document).ready($.proxy(anchorScrolls, 'init'));
    })(window.document, window.history, window.location);

  </script>
  <script type="opt-in" data-name="analytics" data-type="text/javascript" data-src="https://rasa.com/assets/js/u-info.js"></script>
      <div class="webchat-banner webchat-banner--hidden">
        <img src="https://rasa.com/assets/img/demo/sara_avatar.png" class="webchat-banner__avatar" alt="">
        👋 I can help you get started with Rasa and answer your technical questions.
        <button class="webchat-banner__close">
          <i class="fas fa-times"></i> 
        </button>
      </div>
      <div id="webchat">
        <script src="../../_static/rasa-webchat.js"></script>
        <script>
          WebChat.default.init({
            selector: "#webchat",
            initPayload: "/greet",
            socketUrl: "https://website-demo.rasa.com/",
            socketPath: "/socket.io",
            title: "Sara",
            subtitle: "I'm still in development",
            profileAvatar: "https://rasa.com/assets/img/demo/sara_avatar.png",
            showCloseButton: true,
            fullScreenMode: false,
            hideWhenNotConnected: false,
            connectOn: "open",
            autoClearCache: true,
            linksOpenTab: false,
            openLauncherImage: "../../_static/chat-icon.svg",
            customMessageDelay: (message) => {
              return 900 + message.length;
            },
            params: {
              storage: "local"
            },
            onWidgetEvent: {
              onChatOpen: () => {
                const webchatBanner = document.querySelector('.webchat-banner');
                if (webchatBanner) {
                  localStorage.setItem('WEBCHAT_BANNER_DISMISSED', 'true');
                  webchatBanner.classList.add('webchat-banner--hidden');
                }
              }
            }
          });

          try {
            const webchatBanner = document.querySelector('.webchat-banner');

            if (webchatBanner) {
              if (localStorage.getItem('WEBCHAT_BANNER_DISMISSED') !== 'true') {
                webchatBanner.classList.remove('webchat-banner--hidden');
              }

              const webChatBannerClose = document.querySelector('.webchat-banner__close');

              webChatBannerClose && webChatBannerClose.addEventListener('click', function() {
                localStorage.setItem('WEBCHAT_BANNER_DISMISSED', 'true');
                webchatBanner.classList.add('webchat-banner--hidden');
              });
            }
          } catch (e) {}
        </script>
      </div>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>

  </body>
</html>