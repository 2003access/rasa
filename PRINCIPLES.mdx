---
id: principles
sidebar_label: Conversational AI Principles
title: Principles for Building Ethical Conversational Assistants
abstract: You are responsible for the impact that your assistant has on the people that it talks to. We created this guide to help you make this impact benefitial for everyone.
---

When you create a conversational assistant, you are responsible for its impact on the people that it talks to. 
Therefore, you should consider how users might perceive the assistant’s statements, and how a conversation might affect their lives. 
This is not always straightforward, as you typically have little to no knowledge about the background of your users. 
Thus, we created this guide to help you avoid the worst outcomes. 

It is in the best interest of all conversational assistant creators that the public perceives these assistants as helpful and friendly. 
Beyond this, it is also in the best interest of all members of society (including creators) that conversational assistants are neither used for 
harassment nor manipulation. 
Aside from being unethical, such use cases would create a lasting reluctance of users to engage with conversational assistants.

The following four key points should help you use this technology wisely. Please note, however, that these guidelines are only a first step, 
and you should use  your own judgement as well.


## A conversational assistant should not cause users harm

Even though a conversational assistant only exists in the digital world, it can still inflict harm on users. 
For example, conversational assistants often serve as information sources or decision guides. If the information that the assistant provides is inaccurate or misleading, 
users may end up making poor (or even dangerous) decisions based on their interaction with your assistant.
So before you prepare your assistant for production, make sure that all its **responses** and all **information sources** that it may access via [custom actions](./custom-actions.mdx)
are **accurate, well-researched, and not misleading the user in any way**.

## A conversational assistant should not encourage or normalize harmful behaviour from users

Although users have complete freedom in what they can communicate to a conversational assistant, these assistants are designed to only follow pre-defined stories. 
In doing so, a conversational assistant should not try to provoke the user into engaging in harmful behaviour. If for any reason the user decides to engage in this behaviour anyway, 
**the assistant should politely refuse to participate**. Trying to argue with the user will rarely lead to useful results.

## A conversational assistant should always identify itself as one

When asked questions such as “Are you a bot?” or “Are you a human?”, a conversational assistant should always **inform the user that it is a software program**, and not a human. 
This does not mean that conversational assistants can’t be human-like, though.
A good assistant helps users accomplish their goals without pretending to be a human.
In contrast, impostor bots (algorithms that pose as humans) are used as tools for social media manipulation, and this creates a lot of mistrust. 
So instead of misleading users, we should build assistants that make the users' lives better. 

## A conversational assistant should provide users a way to prove its identity

When you design an assistant to represent a company, political party, organization, etc., it is important to allow users to verify that this representation is authorized. 
You can use already existing technologies to do this: For example, you can integrate a conversational assistant to a website served **using HTTPS**, the content of the site 
(and therefore the assistant itself) will be verified as legitimate by a trusted certificate authority. Another example would be to have the conversational assistant use 
a **verified social media account**.

## 

Feel free to reach out to us on [the forum](https://forum.rasa.com/) to discuss your use case! 
We also like to hear your thoughts on these guidelines. 
